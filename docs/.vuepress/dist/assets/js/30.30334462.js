(window.webpackJsonp=window.webpackJsonp||[]).push([[30],{463:function(t,s,a){"use strict";a.r(s);var n=a(2),o=Object(n.a)({},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"content"},[t._m(0),t._v(" "),t._m(1),t._v(" "),t._m(2),t._v(" "),t._m(3),t._v(" "),t._m(4),t._v(" "),t._m(5),t._v(" "),t._m(6),t._v(" "),t._m(7),t._v(" "),t._m(8),t._v(" "),t._m(9),t._v(" "),t._m(10),t._v(" "),t._m(11),t._v(" "),a("p",[t._v("然后吕同学提的预处理的问题 挺好的 词性确实很重要")]),t._v(" "),a("p",[t._v("做了个小统计发现整个词袋里面有5477个词有多重词性的 高等词还有8个词性 so 呢")]),t._v(" "),a("p",[t._v("根据蔡同学的思路 把词频小于10的词过滤掉")]),t._v(" "),a("p",[t._v("26s就跑完了 但感觉 信息丢失的有点多")]),t._v(" "),t._m(12),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("VSM很简单 但hand write起来 还是会有一些问题的")]),t._v(" "),t._m(13),t._v(" "),a("p",[t._v("额 我们拿到的文本 虽然已经分词好了 但 并不是很能用")]),t._v(" "),a("p",[t._v("所以我们需要预处理")]),t._v(" "),t._m(14),t._v(" "),a("p",[t._v("shell 或者说bash脚本 性能对于这种文本处理基本上是秒级的")]),t._v(" "),t._m(15),t._v(" "),a("p",[t._v("可以看出好像前面15位的 代表ArticleId 属于同一个文章")]),t._v(" "),t._m(16),t._v(" "),a("p",[t._v("本来我是想枚举的 但发现 好像26个字母都有 真的恐怖")]),t._v(" "),t._m(17),t._v(" "),a("p",[t._v("然后整理一下就变成了")]),t._v(" "),t._m(18),t._v(" "),a("p",[t._v("性能方面 虽然没打时间 但基本上秒级")]),t._v(" "),a("p",[t._v("附上code")]),t._v(" "),t._m(19),t._m(20),t._v(" "),a("p",[t._v("VSM分三步")]),t._v(" "),t._m(21),t._v(" "),a("p",[t._v("思路很简单")]),t._v(" "),a("p",[t._v("我一开始觉得 TF - IDF计算需要针对每个(article1, article2)进行计算")]),t._v(" "),a("p",[t._v("因为需要对齐 而且最关键要平滑")]),t._v(" "),a("p",[t._v("如果有个词article1没有，article2也没有 如何计算tf的时候因为进行平滑处理 就会占一定比例 这对于那些高词频的 word就不太友好")]),t._v(" "),t._m(22),t._v(" "),a("p",[t._v("看起来 没啥 乘起来就是500w")]),t._v(" "),a("p",[t._v("初始化数组就要1min")]),t._v(" "),a("p",[t._v("于是开了两级多线程")]),t._v(" "),t._m(23),t._v(" "),a("p",[t._v("但效果很差 因为那么多个线程争夺写一个3100✖️3100的numpy.Array 出现了写阻塞现象")]),t._v(" "),a("p",[t._v("通过Activity Monitor观察 实际上线程数只有5.6左右")]),t._v(" "),a("p",[t._v("把numpy.Array换成list 发现效率高了一点 还是不行")]),t._v(" "),a("p",[t._v("于是 想能不能不同时争夺写一个list 直接每一行维护一个list 直接写文件")]),t._v(" "),a("p",[t._v("发现效率提高很多 基本上1s能处理500个数据")]),t._v(" "),a("p",[t._v("那500w需要3h+")]),t._v(" "),t._m(24),t._v(" "),a("p",[t._v("如何先生成3100篇文章的词向量组（tf - idf之后）")]),t._v(" "),a("p",[t._v("再做一次 A.dot(A.T)就可以得到结果")]),t._v(" "),a("p",[t._v("实际效果总耗时215s 约3min 效果较好")]),t._v(" "),a("p",[t._v("然后一次误输出 发现内存中的中间状态数组已经到了4.3G")]),t._v(" "),t._m(25),t._v(" "),t._m(26),t._v(" "),t._m(27),t._v(" "),a("p",[t._v("关于隐语义等考完试再来弄")]),t._v(" "),a("p",[t._v("之前写过一篇关于时序分析相关内容"),a("router-link",{attrs:{to:"/other/rnn.html"}},[t._v("RNN家族的blog")])],1),t._v(" "),t._m(28),a("p",[t._v("祝大家考试顺利🙏")])])},[function(){var t=this.$createElement,s=this._self._c||t;return s("p",[s("code",[this._v("Update 一下")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("跟室友讨论了一下 才发现有一些我以为很理所应当的点 才是优化的关键 "),s("s",[this._v("（当然你们看我的code应该也能看出来 只是没点明白）")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("h1",{attrs:{id:"optimize-point"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#optimize-point","aria-hidden":"true"}},[this._v("#")]),this._v(" Optimize Point")])},function(){var t=this.$createElement,s=this._self._c||t;return s("ol",[s("li",[this._v("不要试图去开大数据量的二维数组")])])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ul",[a("li",[t._v("一旦你初始化一个3k✖️3k的数据 你就会发现即使你只是读一下这个数组就会死慢死慢 还要频繁写入 效率可想而知")]),t._v(" "),a("li",[t._v("正确的姿势 应该是开一个一维的数组 然后每次存入一个3k数组的Index")]),t._v(" "),a("li",[t._v("这么做是有道理的\n"),a("ul",[a("li",[t._v("首先实际数组 相对于动态开起来的")]),t._v(" "),a("li",[t._v("然后我们存在一维数组里的 实际上是Index值 这个会快很多")]),t._v(" "),a("li",[t._v("而且存进去的数组 是"),a("code",[t._v("Immutable")]),t._v("-不(可)更改的 不需要update 这个又会快很多")]),t._v(" "),a("li",[t._v("于是乎 这就是第一个"),a("code",[t._v("bonus Ponit")])])])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("ol",{attrs:{start:"2"}},[s("li",[this._v("要用"),s("code",[this._v("Numpy")]),this._v("的矩阵乘法 🙅不要手写")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("ul",[s("li",[this._v("虽然 我不知道它的内部实现机理 但真的很快")]),this._v(" "),s("li",[this._v("快到瞠目结舌 3k✖️6w的矩阵相乘 10s不到")]),this._v(" "),s("li",[this._v("这 我3k✖️3k遍历一遍就要1min")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("ol",{attrs:{start:"3"}},[s("li",[this._v("可以考虑动态对齐词矩阵 降低词向量维数")])])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ul",[a("li",[t._v("目前我们是按所有文章中词向量维数为所有词向量的长度 大约6w维")]),t._v(" "),a("li",[t._v("如果按每个article为粒度 用动态对齐的方式 可以省很多空间")]),t._v(" "),a("li",[t._v("我之所以没这么干\n"),a("ul",[a("li",[t._v("因为我tf做了smooth操作 所有零项 不能简单的补零 复杂度较高")]),t._v(" "),a("li",[t._v("然后动态补零 就不能用"),a("code",[t._v("numpy")]),t._v("的矩阵乘法 就很伤")])])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("ol",{attrs:{start:"4"}},[s("li",[this._v("另外就是通过开线程实现加速操作")]),this._v(" "),s("li",[s("strong",[this._v("友情提示 内存小的 可能会比较尴尬")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("ul",[s("li",[this._v("在docker里试了一下 在矩阵相乘的时候中间状态 把内存撑爆了 直接killed了")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("剩下的看代码应该就会懂得  "),s("s",[this._v("（这样应该干货多了吧）")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[s("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2018/png/104214/1542257683610-d69de86e-f09c-4fe5-bacf-316f20740d90.png",alt:"图片.png | center | 556x500"}})])},function(){var t=this.$createElement,s=this._self._c||t;return s("h1",{attrs:{id:"preproccess"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#preproccess","aria-hidden":"true"}},[this._v("#")]),this._v(" Preproccess")])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("对于这种文本的预处理 shell是最好的选择 "),s("s",[this._v("(不是php 手动滑稽)")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[s("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2018/png/104214/1542040370549-a935c07c-35cc-4a1c-9142-904905290f4a.png",alt:"图片.png | center | 556x500"}})])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("然后虽然分词过了 但有很多分隔符什么"),s("code",[this._v("/ n")]),this._v(", "),s("code",[this._v("/ c")]),this._v(", "),s("code",[this._v("/ vn")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("然后不只有这些 还有中文标点 什么"),s("code",[this._v("《》")]),this._v(", "),s("code",[this._v("（）")]),this._v("的 也得去掉")])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[s("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2018/png/104214/1542040698469-5334049c-366a-4577-bb60-3dc5c60bf995.png",alt:"图片.png | center | 556x500"}})])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{attrs:{class:"token comment"}},[t._v("# @Author: gunjianpan")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @Date:   2018-11-11 19:11:19")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @Last Modified by:   gunjianpan")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @Last Modified time: 2018-11-11 20:26:47")]),t._v("\n\n"),a("span",{attrs:{class:"token function"}},[t._v("cp")]),t._v(" data.txt "),a("span",{attrs:{class:"token function"}},[t._v("test")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# Set Tag Where File End")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("echo")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'1'")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v(">>")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("test")]),t._v("\n\n"),a("span",{attrs:{class:"token comment"}},[t._v("# According article to jonit string")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @1: Remove blank lines")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @2: split by ‘/x’ or ‘/xx’")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("#     then, jonit string until into another article(recognition by $1)")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @return: Plain text which one plain one artile")]),t._v("\n"),a("span",{attrs:{class:"token function"}},[t._v("sed")]),t._v(" - n "),a("span",{attrs:{class:"token string"}},[t._v("'/./p'")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("test")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("|")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("awk")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'{split("),a("span",{attrs:{class:"token variable"}},[t._v("$0")]),t._v(',a,"/. |/.. ");b="";for(i=3;i<length(a);i++){b=b" "a[i];}if(!last||last==substr('),a("span",{attrs:{class:"token variable"}},[t._v("$1")]),t._v(',0,15)){total=total""b;}else{print substr(total,2);total=b;}last=substr('),a("span",{attrs:{class:"token variable"}},[t._v("$1")]),t._v(",0,15)}'")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v(">>")]),t._v(" test2\n\n"),a("span",{attrs:{class:"token comment"}},[t._v("# Remove Chinese punctuation")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @1: replace punctuation Chinese or English to blank")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @2: replace mutil-blank to one-blank")]),t._v("\n"),a("span",{attrs:{class:"token function"}},[t._v("sed")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'s/[；：，。（）？！《》【】{}“”、——;:,.()?!_]/ /g'")]),t._v(" test2 "),a("span",{attrs:{class:"token operator"}},[t._v("|")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("sed")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'s/[ ][ ]*/ /g'")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v(">>")]),t._v(" test3\n")])])])},function(){var t=this.$createElement,s=this._self._c||t;return s("h1",{attrs:{id:"vsm"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#vsm","aria-hidden":"true"}},[this._v("#")]),this._v(" VSM")])},function(){var t=this.$createElement,s=this._self._c||t;return s("ol",[s("li",[this._v("词长度对齐")]),this._v(" "),s("li",[this._v("TF - IDF（考虑平滑, similarity 方式）")]),this._v(" "),s("li",[this._v("one by one calaulate")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("于是我第一版 就 一个个遍历过去 3100✖️3100 （见"),s("code",[this._v("VSM.vsmCalculate()")]),this._v("）")])},function(){var t=this.$createElement,s=this._self._c||t;return s("ol",[s("li",[this._v("每行为一个线程")]),this._v(" "),s("li",[this._v("每行里面每组similarity计算为一个线程")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[this._v("于是牺牲一下精确度 先按词袋里所有词 对齐词向量（见"),s("code",[this._v("VSM.vsmTest()")]),this._v("）")])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[s("s",[this._v("额 内存小的同学可能就比较尴尬了")])])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[s("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2018/png/104214/1542042196015-6410bc69-0571-4ed1-a7ff-c5459d4240c6.png",alt:"图片.png | center | 556x500"}})])},function(){var t=this.$createElement,s=this._self._c||t;return s("p",[s("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2018/png/104214/1542042273408-e58c733c-1094-4579-a1d8-6d6a63aaad3e.png",alt:"图片.png | center | 556x500"}})])},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{attrs:{class:"token comment"}},[t._v("# -*- coding: utf-8 -*-")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @Author: gunjianpan")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @Date:   2018-11-11 20:27:41")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @Last Modified by:   gunjianpan")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("# @Last Modified time: 2018-11-13 01:07:16")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" math\n"),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),a("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),a("span",{attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" threading\n"),a("span",{attrs:{class:"token keyword"}},[t._v("import")]),t._v(" time\n\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("VSM")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n  handle write vsm 🙉\n  """')]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("__init__")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleMaps "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleNum "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("0")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("0")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resultArray "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wordMaps "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preData"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("preData")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    data prepare\n    """')]),t._v("\n    begin_time"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    file_d "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("open")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v("'test3'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v("'r'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    articles "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" file_d"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("readlines"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    threadings "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleNum "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("len")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("articles"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleMaps "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token boolean"}},[t._v("None")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("range")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleNum"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resultArray "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token boolean"}},[t._v("None")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("range")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleNum"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("range")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleNum"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      work "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" threading"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Thread"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("target"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preDataBasic"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n          articles"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v("'\\n'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rstrip"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      threadings"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("work"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" work "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" threadings"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      work"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" work "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" threadings"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      work"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    end_time"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("preDataBasic")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" article"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" articleId"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    prepare data basic in Threading\n    @param article: article string\n    @param articleId: article id\n    """')]),t._v("\n    words "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" article"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v("' '")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    wordMap "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" words"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" word "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("1")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("else")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("1")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" word "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wordMaps"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wordMaps"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wordMaps"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("1")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("else")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wordMaps"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("1")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleMaps"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("articleId"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" wordMap\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("tfidfTest")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    calculate tdidf value\n    td use Augmented Frequency 0.5 + 0.5 * fre/maxFre\n    """')]),t._v("\n\n    wordlist "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v("wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    maxFrequency "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("max")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("wordlist"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    tf "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token number"}},[t._v("0.5")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("0.5")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v(" index "),a("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v(" maxFrequency "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" wordlist"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    idf "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("math"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleNum "),a("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wordMaps"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v("wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    tfidf "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf "),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v(" idf\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" tfidf\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("tfidf")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    calculate tdidf value\n    td use Augmented Frequency 0.5 + 0.5 * fre/maxFre\n    """')]),t._v("\n\n    wordlist "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v("wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    maxFrequency "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("max")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("wordlist"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    tf "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token number"}},[t._v("0.5")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("0.5")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v(" index "),a("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v(" maxFrequency "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" wordlist"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    idf "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("math"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleNum "),a("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token number"}},[t._v("1")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wordMaps"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v("wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    tfidf "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf "),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v(" idf\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" tfidf "),a("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linalg"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("norm"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tfidf"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("ord")]),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token number"}},[t._v("2")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("preSimilarity")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    align map and then calculate one tfidf\n    """')]),t._v("\n    tempMap "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        index"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" index "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" wordMap "),a("span",{attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wordMaps"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    preMap "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("wordMap"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("tempMap"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resultArray"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tfidf"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preMap"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process "),a("span",{attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("1")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("not")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process "),a("span",{attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("100")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("print")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("vsmTest")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    once to calaulate vsm\n    """')]),t._v("\n    begin_time"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    threadings "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("range")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleNum"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      work "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" threading"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Thread"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("target"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preSimilarity"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n          self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleMaps"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      threadings"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("work"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" work "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" threadings"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      work"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" work "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" threadings"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      work"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    tempMatrix "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resultArray"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    result "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tempMatrix"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tempMatrix"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    df "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    df"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v('"vsm1.csv"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" header"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("False")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    end_time"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("preSimilarityTest")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap1"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap2"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    align map and then calculate one tfidf\n    """')]),t._v("\n    tempMap1 "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        index"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" wordMap1"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" index "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" wordMap1 "),a("span",{attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" wordMap2"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    preMap1 "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("wordMap1"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("**")]),t._v("tempMap1"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tfidfTest"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preMap1"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("similarity")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap1"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap2"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" types"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    calculate similarity by cos distance\n    @Param types: distance calculate type\n                =0 Cos Distance\n                =1 Chebyshev Distance\n                =2 Manhattan Distance\n                =3 Euclidean Distance\n    """')]),t._v("\n    tfidf1 "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preSimilarityTest"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("wordMap1"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap2"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    tfidf2 "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preSimilarityTest"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("wordMap2"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap1"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("not")]),t._v(" types"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tfidf1"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tfidf2"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linalg"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("norm"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tfidf1"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("ord")]),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token number"}},[t._v("2")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linalg"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("norm"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tfidf2"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("ord")]),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token number"}},[t._v("2")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" types "),a("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("1")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token builtin"}},[t._v("abs")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tfidf1 "),a("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v(" tfidf2"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token builtin"}},[t._v("max")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" types "),a("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("2")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token builtin"}},[t._v("abs")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tfidf1 "),a("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v(" tfidf2"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" types "),a("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("3")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linalg"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("norm"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tfidf1 "),a("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v(" tfidf2"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("else")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nonzero"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tfidf1 "),a("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v(" tfidf2"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token number"}},[t._v("0")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token number"}},[t._v("0")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("vsmCalculate")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    calculate vsm\n    """')]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("#: todo write block")]),t._v("\n    begin_time"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    threadings "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index1 "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("range")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleNum"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      work "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" threading"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Thread"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("target"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vsmThread"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("index1"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      threadings"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("work"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" work "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" threadings"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      work"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" work "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" threadings"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      work"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    end_time"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("vsmThread")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index1"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    vsm threading\n    """')]),t._v("\n    nowarticle "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleMaps"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index1"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    tempResult "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index2 "),a("span",{attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{attrs:{class:"token builtin"}},[t._v("range")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("index1"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleNum"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      tempResult"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vsmPre"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n          nowarticle"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("articleMaps"),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index2"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    df "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("index1"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tempResult"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    df"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token string"}},[t._v("'vsm.csv'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mode"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token string"}},[t._v("'a'")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" header"),a("span",{attrs:{class:"token operator"}},[t._v("=")]),a("span",{attrs:{class:"token boolean"}},[t._v("False")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("vsmPre")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap1"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap2"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    load data to result\n    prevent read block\n    """')]),t._v("\n\n    self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process "),a("span",{attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("1")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("not")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process "),a("span",{attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("100")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),a("span",{attrs:{class:"token keyword"}},[t._v("print")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("similarity"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("wordMap1"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordMap2"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("0")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\nstart "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("0")]),t._v("\n\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("begin_time")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("global")]),t._v(" start\n  start "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("time"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("end_time")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),a("span",{attrs:{class:"token keyword"}},[t._v("print")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("time"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("time"),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v(" start"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])}],!1,null,null,null);o.options.__file="vsm.md";s.default=o.exports}}]);