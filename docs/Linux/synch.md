---
title: æ•°ä¸€æ•°Linuxä¸­æœ‰å¤šå°‘ç§çº¿ç¨‹åŒæ­¥ç­–ç•¥-ã€Linux æºç è§£æï¼ˆäºŒï¼‰ã€
date: 2019-03-24 16:11:31
tags: [Linux/Thread]
description: Linux çº¿ç¨‹åŒæ­¥
---

æœ¬æ¥è¿™ç¯‡åº”è¯¥æ˜¯ä¸Šå‘¨å‘çš„ï¼Œæ‹–å»¶ç—‡åˆçŠ¯äº† ğŸ™ˆ

ä¸Šä¸€ç¯‡ä¸»è¦è®¨è®ºäº†[Linux çº¿ç¨‹çš„è°ƒåº¦ç®—æ³•](https://wyydsb.xin/Linux/schedule.html)

è¿™ç¯‡æ¥è°ˆè°ˆçº¿ç¨‹é—´çš„åŒæ­¥é—®é¢˜ï¼Œæš‚æ—¶ä¸åŒ…æ‹¬`IPC(InterProcess Communication)`é—®é¢˜ï¼ŒIPC è¿˜æ˜¯å¾ˆæœ‰è¶£çš„ã€‚

æœ‰è¶£çš„äº‹æƒ…å°±è¦æ…¢æ…¢å“å¯¹å§ï¼Œç•™åˆ°ä¸‹æ¬¡å†æ¥è°ˆ ğŸŒšï¼ˆä¸»è¦å‡†å¤‡ä¸è¿‡æ¥ hhh å¤ªçœŸå®äº†ï¼‰

<center><img width="150" src="https://cdn.nlark.com/yuque/0/2019/png/104214/1553443429332-c9e3790c-cc68-446b-87a2-a9c2645f679a.png"></center>

PS: ä»¥ä¸‹è§£æçš„ Linux kernel ç‰ˆæœ¬å·ä¸º`4.19.25`

> Thread synchronization

## Motivation

ä¸ºä»€ä¹ˆçº¿ç¨‹ä¹‹é—´éœ€è¦åŒæ­¥ï¼Ÿ

ä¸€ä¸ªåŸå› ï¼ŒåŒä¸€ä¸ªçˆ¶è¿›ç¨‹ä¸‹çš„æ‰€æœ‰å­çº¿ç¨‹å…±äº«åŒä¸€ä¸ª PCï¼ŒåŒä¸€ä¸ªå¯„å­˜å™¨ï¼ŒåŒä¸€ä¸ªå…±äº«åº“(åŒä¸€ç‰‡å¤©ç©º)

æ‰€ä»¥å½“å¤šä¸ªå­çº¿ç¨‹åŒæ—¶å¯¹åŒä¸€ä¸ªå˜é‡è¿›è¡Œæ“ä½œçš„æ—¶å€™ï¼Œå°±å¾ˆæœ‰å¯èƒ½å‡ºç°çƒ­ç‚¹ï¼Œç”šè‡³é”™è¯¯æƒ…å†µï¼Œè¿™å°±æ˜¯åŒæ­¥é—®é¢˜ã€‚

å¦å¤–ä¸€ä¸ªåŸå› ï¼Œå¾ˆå¤šæ—¶å€™çº¿ç¨‹ä¹‹é—´æ‰§è¡Œæƒ…å†µå®é™…ä¸Šæ˜¯æœ‰ä¸€å®šé¡ºåºçš„ï¼Œä¸‹ä¸€ä¸ªçº¿ç¨‹éœ€è¦çŸ¥é“ä¸Šä¸€ä¸ªçº¿ç¨‹æœ‰æ²¡æœ‰å®Œæˆæ‰§è¡Œä»»åŠ¡ã€‚

å½“ç„¶çº¿ç¨‹æƒé™æ²¡æœ‰é‚£ä¹ˆå¤§ï¼Œè¿™äº›äº‹æƒ…éƒ½æ˜¯è°ƒåº¦ç¨‹åºæ¥åšçš„ï¼Œä½†çº¿ç¨‹æœ‰æ„ŸçŸ¥ä¸Šä¸€ä¸ªçº¿ç¨‹å®Œæˆä¸å¦çš„éœ€æ±‚ï¼Œè¿™å°±æ˜¯äº’æ–¥é—®é¢˜ã€‚

æ‰€ä»¥ï¼Œæ€»çš„è€Œè¨€ï¼Œçº¿ç¨‹åŒæ­¥ä¸»è¦è§£å†³çš„æ˜¯åŒæ­¥äº’æ–¥é—®é¢˜ã€‚

è‡³äºæ€ä¹ˆè§£å†³ï¼Œå¸¸è§çš„å¥—è·¯ä¸»è¦æ˜¯åœ¨æ ˆä¸­è®¾ç«‹ä¸€ä¸ªåŸå­å˜é‡ï¼Œé€šè¿‡æŠ¢å è¿™ä¸ªå…¨éƒ¨å˜é‡å®ç°åŒæ­¥äº’æ–¥ã€‚

å…·ä½“è€Œè¨€ï¼Œæœ‰`äº’æ–¥é‡mutex`, `é”Lock`, `è¯»å†™é”rwlock`, `æ¡ä»¶å˜é‡Condition`, `å±éšœBarrier` etc.

## Souce code

> è¿™ä¸€éƒ¨åˆ†ä»£ç æ¯”è¾ƒå¤šï¼Œæœ‰äº›è¿˜æ¯”è¾ƒæ™¦æ¶©ï¼ŒLinux kernel4 ä»¥åçš„ä»£ç ç›¸è¾ƒäº 2.Ã— ç‰ˆæœ¬è¿˜æœ‰æ¯”è¾ƒå¤§çš„æ”¹åŠ¨
> ç„¶ååœ¨å·¥ç¨‹ä¸­ï¼Œè¿™ä¸€éƒ¨åˆ†è¿˜æ˜¯å¾ˆæœ‰ç”¨çš„ï¼Œæ¯”å¦‚æ‰€æœ‰çº¿ç¨‹å®‰å…¨éƒ½æ˜¯åŸºäºäº’æ–¥é‡è¿™ä¸ªæ¦‚å¿µå®ç°çš„ï¼Œå†æ¯”å¦‚è¯´å†™ä¸ª redis é” etc.
> æŒæ¡è¿™éƒ¨åˆ†ä¼šå¯¹ä½ ç»´æŠ¤å¤šçº¿ç¨‹é—®é¢˜æœ‰æ‰€å¸®åŠ©ï¼ï¼ï¼

Linux çš„çº¿ç¨‹åŒæ­¥æœºåˆ¶å’Œ Nachos ä¸­ä½¿ç”¨çš„æœºåˆ¶(ä¿¡å·é‡ï¼Œé”ï¼Œæ¡ä»¶å˜é‡)åŸºæœ¬ä¸€è‡´ã€‚é‡‡ç”¨äº†äº’æ–¥é‡ mutexï¼Œæ¡ä»¶å˜é‡ï¼Œä¿¡å·é‡ï¼Œè¯»å†™é”ã€‚

### `Mutex`

Linux ä¸‹é€šè¿‡å£°æ˜ä¸€ä¸ª Mutex çš„ç±»æ¥å®ç°äº’æ–¥é‡çš„å®ç°ã€‚å¦å¤–è¿˜å£°æ˜äº†ä¸€ä¸ª`ww_mutex`(wound/wait)æ¥é¿å…æ­»é”

Linux kerenal ä¸­å…³äº Mutex struct çš„ä»£ç åœ¨`<include/linux/mutex.h>`ä¸­

```c
struct mutex {
        atomic_long_t           owner;     // mutex è·å¾—çš„owner ID
                                           // è‹¥==0, åˆ™mutexæœªè¢«å ç”¨;
                                           // è‹¥> 0, åˆ™mutexè¢«æ­¤ownerIdå ç”¨ï¼Œ
                                           // åªèƒ½ç”±å½“å‰ownerè§£mutex
        spinlock_t              wait_lock; // è‡ªæ—‹é”ç±»å‹
#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
        struct optimistic_spin_queue osq;  /* Spinner MCS lock */
#endif
        struct list_head        wait_list; // ç­‰å¾…é˜Ÿåˆ—
#ifdef CONFIG_DEBUG_MUTEXES
        void                    *magic;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
        struct lockdep_map      dep_map;
#endif
};
```

ä¸Šé¢çš„ struct ç”¨ä¸€ä¸ªåŸå­å˜é‡`owner`æ¥å®ç° mutex çš„äº’æ–¥æ•ˆæœ, è¿™é‡Œå·²ç»å’Œ kernel 2.Ã— ç‰ˆæœ¬ä¸ä¸€æ ·äº†ã€‚

å½“ owner ä¸º 0 æ—¶ï¼Œè¡¨ç¤ºè¿™ä¸ª mutex è¿˜æœªè¢«å ç”¨ã€‚å½“ mutex ä¸ä¸ºé›¶çš„æ—¶å€™ï¼Œåªèƒ½ç”± id == owner çš„çº¿ç¨‹è§£é™¤å ç”¨

å¦å¤–å®šä¹‰äº†ä¸€ä¸ª`wait_list`ç”¨äºå­˜å‚¨è¢« sleep çš„ thread

è¿™éƒ¨åˆ†ä»£ç å’Œ nachos ä¸­ Semaphore çš„è®¾è®¡åŸºæœ¬ä¸€è‡´

è€Œå…·ä½“å®ç° mutex çš„ä»£ç ä½äº`<kernel/locking/mutex.c>`ä¸­

`__mutex_init`å‡½æ•°ä¸»è¦åšä¸€äº›å˜é‡å£°æ˜å’Œåˆå§‹åŒ–çš„å·¥ä½œã€‚

```c
void
__mutex_init(struct mutex *lock, const char *name, struct lock_class_key *key)
{
        atomic_long_set(&lock->owner, 0);   // init atomic å˜é‡ owner
        spin_lock_init(&lock->wait_lock);   // init è‡ªæ—‹é”ç±»å‹å˜é‡
        INIT_LIST_HEAD(&lock->wait_list);   // init ç­‰å¾…é˜Ÿåˆ—å˜é‡
#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
        osq_lock_init(&lock->osq);
#endif
        debug_mutex_init(lock, name, key);
}
```

ä»¥åŠ é”ä¸ºä¾‹ï¼Œè°ƒç”¨çš„çš„æ˜¯ mutex_lock å‡½æ•°ã€‚

```c
void __sched mutex_lock(struct mutex *lock)
{
        might_sleep();                       // æ‰“å°å †æ ˆ debug sleep

        if (!__mutex_trylock_fast(lock))     // atomicè·å¾—owner, å¦‚æœèƒ½
                __mutex_lock_slowpath(lock); //
}
EXPORT_SYMBOL(mutex_lock);
#endif
```

å…¶ä¸­ï¼Œmight_sleep()æ˜¯ä¸€ä¸ªå…¨å±€ Linux APIï¼Œä¸»è¦ç”¨äºåœ¨ä¸­æ–­æ—¶å€™ï¼Œdebug æ‰“å° context å †æ ˆï¼Œè¿™ä¸ª API åœ¨åé¢è¢«å¹¿æ³›ä½¿ç”¨ã€‚

`__mutex_trylock_fast(lock)` æ˜¯ä¸€ä¸ªå»è·å– lock çš„ owner çš„å‡½æ•°ï¼Œå¦‚æœèƒ½è·å–åˆ™è¿”å› true

```c
static __always_inline bool __mutex_trylock_fast(struct mutex *lock)
{ww_acquire_ctx
        unsigned long curr = (unsigned long)current;
        unsigned long zero = 0UL;
        if (atomic_long_try_cmpxchg_acquire(&lock->owner, &zero, curr))  // è·å–owner
                return true;
        return false;
}
```

å¦‚æœæœ‰æƒé™è·å– owner åˆ™

```c
static noinline void __sched
__mutex_lock_slowpath(struct mutex *lock)
{
        __mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);  // è°ƒç”¨__mutex_lock
}
```

ç„¶åå†åµŒå¥—è°ƒç”¨ï¼Œä¸çŸ¥é“æ˜¯ä¸ºäº†ä»€ä¹ˆï¼Œå†™äº†é‚£ä¹ˆå¤šå±‚ï¼ˆå¯èƒ½æ˜¯æœ‰åˆ«çš„åœ°æ–¹ å¤ç”¨åˆ°äº†ï¼‰

```c
static int __sched
__mutex_lock(struct mutex *lock, long state, unsigned int subclass,
             struct lockdep_map *nest_lock, unsigned long ip)
{
        // è°ƒç”¨__mutex_lock_common
        return __mutex_lock_common(lock, state, subclass, nest_lock, ip, NULL, false);
}
```

ç„¶åå°±åˆ°äº† Linux çœŸæ­£å¤„ç† mock_lock çš„åœ°æ–¹

```c
static __always_inline int __schedw
__mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,// lock TASK_UNINTERRUPTIBLE 0
                    struct lockdep_map *nest_lock, unsigned long ip,      // NULL _RET_IP_
                    struct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx) // NULL false
{
        struct mutex_waiter waiter;
        bool first = false;
        struct ww_mutex *ww;     // ww = wound/wait mutex ç”¨äºæ­»é”æ£€éªŒ
        int ret;

        might_sleep(); // ä¸€æ ·çš„å»æ‰“å°contextçš„å †æ ˆ

        ww = container_of(lock, struct ww_mutex, base); // è·å¾—ww_mutex
        if (use_ww_ctx && ww_ctx) {                     // mutet_lockè¿›ä¸åˆ°è¿™ä¸ª,ww_mutex_lockæœ‰å¯èƒ½è¿›
                if (unlikely(ww_ctx == READ_ONCE(ww->ctx))) // ww_mutexè·å¾—çš„ctxå’Œéœ€è¦çš„ctxå¯¹æ¯”
                        return -EALREADY;

                /*
                 * Reset the wounded flag after a kill. No other process can
                 * race and wound us here since they can't have a valid owner
                 * pointer if we don't have any locks held.
                 */
                if (ww_ctx->acquired == 0)   // å¦‚æœww_ctxæ²¡æœ‰è¢«è·å¾— åˆ™é‡è®¾wounded ä½
                        ww_ctx->wounded = 0;
        }

        preempt_disable();  // è®¾ç½®ä¸å¯æŠ¢å 
        mutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip); // æ£€æŸ¥mutex éœ€è¦çš„æ¡ä»¶

        if (__mutex_trylock(lock) ||                                  // å°è¯•ä¸Šlock
            mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, NULL)) {  // å°è¯•ä¸Šä¹è§‚é”
                /* got the lock, yay! */
                lock_acquired(&lock->dep_map, ip);                    // ä¸Šlock
                if (use_ww_ctx && ww_ctx)                             // ww_mutex_lockæ—¶
                        ww_mutex_set_context_fastpath(ww, ww_ctx);    // è®¾ç½®ä¸Šä¸‹æ–‡path
                preempt_enable();                                     // è§£é™¤ä¸å¯æŠ¢å 
                return 0;
        }
        spin_lock(&lock->wait_lock); // å¯¹ç­‰å¾…é˜Ÿåˆ—ä¸Šè‡ªæ—‹é”
        /*
         * After waiting to acquire the wait_lock, try again.
         */
        if (__mutex_trylock(lock)) {                                 // é‚£å†è¯•è¯•å‘— hhh
                if (use_ww_ctx && ww_ctx)
                        __ww_mutex_check_waiters(lock, ww_ctx);

                goto skip_wait;
        }

        debug_mutex_lock_common(lock, &waiter); // æ‰ä¸€ä¸‹debugæ¨¡å¼ä¸‹mutet_lock_common

        lock_contended(&lock->dep_map, ip);     // å»ç­‰é”

        if (!use_ww_ctx) {                      // mutex_lockæ—¶å€™
                /* add waiting tasks to the end of the waitqueue (FIFO): */
                __mutex_add_waiter(lock, &waiter, &lock->wait_list); // åŠ åˆ°wait_queue


#ifdef CONFIG_DEBUG_MUTEXES
                waiter.ww_ctx = MUTEX_POISON_WW_CTX;
#endif
        } else {
                /*
                 * Add in stamp order, waking up waiters that must kill
                 * themselves.
                 */
                ret = __ww_mutex_add_waiter(&waiter, lock, ww_ctx); // åŠ åˆ°ww_mutexçš„wait_queue
                if (ret)
                        goto err_early_kill;

                waiter.ww_ctx = ww_ctx;
        }

        waiter.task = current;

        set_current_state(state);  // è®¾ç½®state
        for (;;) {                // åšäº†ä¸€ä¸ªè‡ªæ—‹æ“ä½œ retry lock
                /*
                 * Once we hold wait_lock, we're serialized against
                 * mutex_unlock() handing the lock off to us, do a trylock
                 * before testing the error conditions to make sure we pick up
                 * the handoff.
                 */
                if (__mutex_trylock(lock))  // ç­‰åˆ°äº†
                        goto acquired;

                /*
                 * Check for signals and kill conditions while holding
                 * wait_lock. This ensures the lock cancellation is ordered
                 * against mutex_unlock() and wake-ups do not go missing.
                 */
                if (unlikely(signal_pending_state(state, current))) { // if stateä¸å¯¹
                        ret = -EINTR;
                        goto err;
                }

                if (use_ww_ctx && ww_ctx) {  // å¦‚æœæ˜¯ww_mutex ä¸” wait_queue æœ‰éœ€è¦è¢«killæ‰çš„
                        ret = __ww_mutex_check_kill(lock, &waiter, ww_ctx);
                        if (ret)
                                goto err;
                }

                spin_unlock(&lock->wait_lock); // è§£è‡ªæ—‹é”
                schedule_preempt_disabled();   // è§£é™¤ä¸å¯æŠ¢å 

                /*
                 * ww_mutex needs to always recheck its position since its waiter
                 * list is not FIFO ordered.
                 */
                if ((use_ww_ctx && ww_ctx) || !first) {
                        first = __mutex_waiter_is_first(lock, &waiter);
                        if (first)
                                __mutex_set_flag(lock, MUTEX_FLAG_HANDOFF);
                }

                set_current_state(state); // update state
                /*
                 * Here we order against unlock; we must either see it change
                 * state back to RUNNING and fall through the next schedule(),
                 * or we must see its unlock and acquire.
                 */
                if (__mutex_trylock(lock) || // å†è¯•ä¸€æ¬¡
                    (first && mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, &waiter)))
                        break;

                spin_lock(&lock->wait_lock);
        }
        spin_lock(&lock->wait_lock);
acquired:
        __set_current_state(TASK_RUNNING);

        if (use_ww_ctx && ww_ctx) {
                /*
                 * Wound-Wait; we stole the lock (!first_waiter), check the
                 * waiters as anyone might want to wound us.
                 */
                if (!ww_ctx->is_wait_die &&
                    !__mutex_waiter_is_first(lock, &waiter))
                        __ww_mutex_check_waiters(lock, ww_ctx);
        }

        mutex_remove_waiter(lock, &waiter, current); // ä»ç­‰å¾…é˜Ÿåˆ—ä¸­remove
        if (likely(list_empty(&lock->wait_list)))
                __mutex_clear_flag(lock, MUTEX_FLAGS); // æ¸…é™¤flag

        debug_mutex_free_waiter(&waiter);

skip_wait:
        /* got the lock - cleanup and rejoice! */
        lock_acquired(&lock->dep_map, ip);

        if (use_ww_ctx && ww_ctx)
                ww_mutex_lock_acquired(ww, ww_ctx);

        spin_unlock(&lock->wait_lock); // cleanup
        preempt_enable();
        return 0;

err:
        __set_current_state(TASK_RUNNING);
        mutex_remove_waiter(lock, &waiter, current);
err_early_kill:
        spin_unlock(&lock->wait_lock);
        debug_mutex_free_waiter(&waiter);
        mutex_release(&lock->dep_map, 1, ip);
        preempt_enable();
        return ret;
}
```

ä¸Šé¢çš„`__mutex_common`è¢«`mutex_lock`ï¼Œ`ww_mutex_lock`ä¸¤ä¸ªå‡½æ•°å¤ç”¨

`use_ww_ctx` && `ww_ctx`è¿™ä¸¤ä¸ªå˜é‡å°±æ˜¯ç”¨æ¥åˆ¤æ–­åˆ°åº•æ˜¯è¢«å“ªä¸ªå‡½æ•°å¤ç”¨äº†

ç„¶åå‡½æ•°å¾ˆå¤šé€»è¾‘éƒ½æ˜¯ä¸ºäº†å‡å°‘ç­‰å¾…æ—¶é—´ï¼Œç”¨äº†å¤šæ¬¡è‡ªæ—‹é”è¿›è¡Œç­‰å¾…ï¼Œç›´åˆ°å¤šæ¬¡å°è¯•ä¹‹åè¿˜ä¸èƒ½ä¸Šé”çš„æ—¶å€™æ‰çœŸæ­£å» sleep ç­‰å¾…

è¿™æ ·çš„æ“ä½œè™½ç„¶å¯èƒ½ä¼šå¢å¤§å•æ¬¡ä¸Šé”æ—¶é—´ï¼Œä½†ç›¸æ¯”äº¤æ¢ä¸Šä¸‹æ–‡ Context çš„ä»£ä»·è‚¯å®šæ˜¯å¾ˆçœäº†

#### `è‡ªæ—‹é” spinlock`

è‡ªæ—‹é”ï¼Œå°±æ˜¯ä¸€ç§åå¤é‡è¯•çš„é”ï¼Œå› ä¸ºå®é™…ç”Ÿäº§è¿‡ç¨‹ä¸­ï¼Œç»å¸¸ä¼šæœ‰ç¨å¾®ç­‰ä¸€ç­‰è¿™ä¸ªäº’æ–¥é‡å°±è§£é™¤çš„æƒ…å†µ

æ‰€ä»¥è‡ªæ—‹é”åœ¨å·¥ç¨‹ä¸­ç”¨å¤„è¿˜æ˜¯å¾ˆå¤§çš„ï¼Œå¾ˆå¤š java ç¨‹åºéƒ½è¦å†™ spinlock

Spinlock ç›¸å…³ä»£ç åœ¨`<include/linux/spinlock_api_smp.h>`ä¸­

```c
static inline int __raw_spin_trylock(raw_spinlock_t *lock)
{
        preempt_disable(); // è®¾ç½®ä¸å¯æŠ¢å 
        if (do_raw_spin_trylock(lock)) {  // å°è¯•è·å¾—è‡ªæ—‹é”
                spin_acquire(&lock->dep_map, 0, 1, _RET_IP_); // è·å¾—è‡ªæ—‹é”
                return 1;
        }
        preempt_enable(); // æ¥è§¦ä¸å¯æŠ¢å 
        return 0;
}
```

å…¶ä¸­ spin_acquire å®šä¹‰åœ¨`<include/linux/lockdep.h>`

```c
#define spin_acquire(l, s, t, i)                lock_acquire_exclusive(l, s, t, NULL, i)
#define lock_acquire_exclusive(l, s, t, n, i)           lock_acquire(l, s, t, 0, 1, n, i)
```

è€Œ lock_acquire()å®ç°çš„ä»£ç åœ¨`<kernel/locking/lockdep.c>`

```c
void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
                          int trylock, int read, int check,
                          struct lockdep_map *nest_lock, unsigned long ip)
{
        unsigned long flags;

        if (unlikely(current->lockdep_recursion)) // å¦‚æœé”çš„é€’å½’æ·±åº¦æ ‡å¿—ä½!=0
                return;

        raw_local_irq_save(flags); // åˆ·ä¸€ä¸‹flagsåˆ°disk
        check_flags(flags); // æ£€æŸ¥flag

        current->lockdep_recursion = 1; // äº’æ–¥
        trace_lock_acquire(lock, subclass, trylock, read, check, nest_lock, ip); // è¿½è¸ªé”è·å¾— æ‰“å°æ—¥å¿—
        __lock_acquire(lock, subclass, trylock, read, check,
                       irqs_disabled_flags(flags), nest_lock, ip, 0, 0);  // lock acquire
        current->lockdep_recursion = 0; // è§£é™¤äº’æ–¥
        raw_local_irq_restore(flags); // å†åˆ·ä¸€ä¸‹flags
}
EXPORT_SYMBOL_GPL(lock_acquire);
```

ç„¶åå…·ä½“å®ç°çš„æ—¶å€™ï¼Œè°ƒç”¨åˆ°`__lock_acquire()`

```c
static int __lock_acquire(struct lockdep_map *lock, unsigned int subclass,
                          int trylock, int read, int check, int hardirqs_off,
                          struct lockdep_map *nest_lock, unsigned long ip,
                          int references, int pin_count)
{
        struct task_struct *curr = current;
        struct lock_class *class = NULL;
        struct held_lock *hlock;
        unsigned int depth;
        int chain_head = 0;
        int class_idx;
        u64 chain_key;

        if (subclass < NR_LOCKDEP_CACHING_CLASSES)
                class = lock->class_cache[subclass]; // æ‰¾åˆ°cache
        /*
         * Not cached?
         */
        if (unlikely(!class)) {
                class = register_lock_class(lock, subclass, 0); // æ³¨å†Œlock
                if (!class)
                        return 0;
        }
        atomic_inc((atomic_t *)&class->ops); // åŸå­æ“ä½œè·å¾—class æ“ä½œç¬¦
        if (very_verbose(class)) {
                printk("\nacquire class [%px] %s", class->key, class->name);
                if (class->name_version > 1)
                        printk(KERN_CONT "#%d", class->name_version);
                printk(KERN_CONT "\n");
                dump_stack();
        }
        depth = curr->lockdep_depth;  // init depth

        if (DEBUG_LOCKS_WARN_ON(depth >= MAX_LOCK_DEPTH)) // stackæ·±åº¦æº¢å‡º
                return 0;

        class_idx = class - lock_classes + 1;

        if (depth) {
                hlock = curr->held_locks + depth - 1;
                if (hlock->class_idx == class_idx && nest_lock) {
                        if (hlock->references) {
                                /*
                                 * Check: unsigned int references:12, overflow.
                                 */
                                if (DEBUG_LOCKS_WARN_ON(hlock->references == (1 << 12)-1)) // 2^12 - 1
                                        return 0;

                                hlock->references++;
                        } else {
                                hlock->references = 2;
                        }

                        return 1;
                }
        }

        hlock = curr->held_locks + depth;
        if (DEBUG_LOCKS_WARN_ON(!class))
                return 0;
        hlock->class_idx = class_idx; // è®°å½•hlockä¿¡æ¯
        hlock->acquire_ip = ip;
        hlock->instance = lock;
        hlock->nest_lock = nest_lock;
        hlock->irq_context = task_irq_context(curr);
        hlock->trylock = trylock;
        hlock->read = read;
        hlock->check = check;
        hlock->hardirqs_off = !!hardirqs_off;
        hlock->references = references;
#ifdef CONFIG_LOCK_STAT
        hlock->waittime_stamp = 0;
        hlock->holdtime_stamp = lockstat_clock();
#endif
        hlock->pin_count = pin_count;

        if (check && !mark_irqflags(curr, hlock))
                return 0;

        /* mark it as used: */
        if (!mark_lock(curr, hlock, LOCK_USED))
                return 0;

        if (DEBUG_LOCKS_WARN_ON(class_idx > MAX_LOCKDEP_KEYS)) // åˆæº¢å‡ºäº†
                return 0;

        chain_key = curr->curr_chain_key;
        if (!depth) {
                /*
                 * How can we have a chain hash when we ain't got no keys?!
                 */
                if (DEBUG_LOCKS_WARN_ON(chain_key != 0))
                        return 0;
                chain_head = 1;
        }

        hlock->prev_chain_key = chain_key;
        if (separate_irq_context(curr, hlock)) {
                chain_key = 0;
                chain_head = 1;
       }
        chain_key = iterate_chain_key(chain_key, class_idx);
        curr->curr_chain_key = chain_key;
        curr->lockdep_depth++;
        check_chain_key(curr);
        return 1;
}
```

`__lock_acquire()`è¢«`spin_lock` å’Œ `mutex_lock`ä¸¤ä¸ª class è°ƒç”¨

å®é™…ä¸Šå®ƒçš„æ“ä½œå¯¹è±¡ä¸æ˜¯å¯¹å•ä¸€ class åŠ é”ï¼Œæ˜¯å¯¹ä¸€ä¸ªé”ç±»çš„åŠ é”

è¿™é‡Œä¸ºäº†é™ä½ lockdep çš„æœç´¢æ¶ˆè€—ï¼Œç”¨äº†ä¸€ä¸ª cache

å¯¹äºé‚£äº›åå¤åŠ æ”¾é”çš„éƒ¨åˆ†æœ‰ä¸å°çš„æ€§èƒ½ä¸Šçš„æå‡

- `è¯»å†™é”rwlock`

è¯»å†™é”çš„ä¸»è¦ç›®çš„å°±æ˜¯å®ç°æŸä¸€ç§çŠ¶æ€çš„å¹¶å‘æ€§

- `æ¡ä»¶å˜é‡ Condition`

æ¡ä»¶å˜é‡åˆ™æ˜¯ä¸ºäº†å®ç°çº¿ç¨‹çš„æ‰¹å¤„ç†ï¼Œä¸€ä¸ªä¸ª batch æ‰§è¡Œï¼Œå®šä¹‰äº†å•ä¸ªå”¤é†’ & å¹¿æ’­å”¤é†’ä¸¤ç§æ–¹å¼

- `å±éšœ barrier`

å±éšœçš„ä½œç”¨å°±å¾ˆåƒä¸¤é˜¶æ®µé”åè®®ï¼Œç¬¬ä¸€é˜¶æ®µåªèƒ½ç­‰å¾…ï¼Œç¬¬äºŒé˜¶æ®µåªèƒ½è¿è¡Œ

å½“æœªè¾¾åˆ°å±éšœçº¦å®šçš„ä¸Šé™æ—¶ï¼Œé€šè¿‡æ¡ä»¶å˜é‡å®ç°è¿›å…¥ wait_queue

å½“è¾¾åˆ°å±éšœä¸Šé™çš„æ—¶å€™ï¼Œé€šè¿‡å¹¿æ’­ä¸€æ¬¡æ€§å”¤é†’
