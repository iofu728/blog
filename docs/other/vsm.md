---
pageClass: custom - page - class
---

# å…³äºVSMæ€§èƒ½ä¼˜åŒ–çš„æ€è€ƒğŸ¤”

VSMå¾ˆç®€å• ä½†hand writeèµ·æ¥ è¿˜æ˜¯ä¼šæœ‰ä¸€äº›é—®é¢˜çš„

# Preproccess

é¢ æˆ‘ä»¬æ‹¿åˆ°çš„æ–‡æœ¬ è™½ç„¶å·²ç»åˆ†è¯å¥½äº† ä½† å¹¶ä¸æ˜¯å¾ˆèƒ½ç”¨

æ‰€ä»¥æˆ‘ä»¬éœ€è¦é¢„å¤„ç†

å¯¹äºè¿™ç§æ–‡æœ¬çš„é¢„å¤„ç† shellæ˜¯æœ€å¥½çš„é€‰æ‹© ~~(ä¸æ˜¯php æ‰‹åŠ¨æ»‘ç¨½)~~

shell æˆ–è€…è¯´bashè„šæœ¬ æ€§èƒ½å¯¹äºè¿™ç§æ–‡æœ¬å¤„ç†åŸºæœ¬ä¸Šæ˜¯ç§’çº§çš„

![å›¾ç‰‡.png | center | 556x500](https://cdn.nlark.com/yuque/0/2018/png/104214/1542040370549-a935c07c-35cc-4a1c-9142-904905290f4a.png "")

å¯ä»¥çœ‹å‡ºå¥½åƒå‰é¢15ä½çš„ ä»£è¡¨ArticleId å±äºåŒä¸€ä¸ªæ–‡ç« 

ç„¶åè™½ç„¶åˆ†è¯è¿‡äº† ä½†æœ‰å¾ˆå¤šåˆ†éš”ç¬¦ä»€ä¹ˆ` / n`, ` / c`, ` / vn`

æœ¬æ¥æˆ‘æ˜¯æƒ³æšä¸¾çš„ ä½†å‘ç° å¥½åƒ26ä¸ªå­—æ¯éƒ½æœ‰ çœŸçš„ææ€–

ç„¶åä¸åªæœ‰è¿™äº› è¿˜æœ‰ä¸­æ–‡æ ‡ç‚¹ ä»€ä¹ˆ`ã€Šã€‹`, `ï¼ˆï¼‰`çš„ ä¹Ÿå¾—å»æ‰

ç„¶åæ•´ç†ä¸€ä¸‹å°±å˜æˆäº†

![å›¾ç‰‡.png | center | 556x500](https://cdn.nlark.com/yuque/0/2018/png/104214/1542040698469-5334049c-366a-4577-bb60-3dc5c60bf995.png "")

æ€§èƒ½æ–¹é¢ è™½ç„¶æ²¡æ‰“æ—¶é—´ ä½†åŸºæœ¬ä¸Šç§’çº§

é™„ä¸Šcode

```bash
# @Author: gunjianpan
# @Date:   2018-11-11 19:11:19
# @Last Modified by:   gunjianpan
# @Last Modified time: 2018-11-11 20:26:47

cp data.txt test
# Set Tag Where File End
echo '1' >> test

# According article to jonit string
# @1: Remove blank lines
# @2: split by â€˜/xâ€™ or â€˜/xxâ€™
#     then, jonit string until into another article(recognition by $1)
# @return: Plain text which one plain one artile
sed - n '/./p' test | awk '{split($0,a,"/. |/.. ");b="";for(i=3;i<length(a);i++){b=b" "a[i];}if(!last||last==substr($1,0,15)){total=total""b;}else{print substr(total,2);total=b;}last=substr($1,0,15)}' >> test2

# Remove Chinese punctuation
# @1: replace punctuation Chinese or English to blank
# @2: replace mutil-blank to one-blank
sed 's/[ï¼›ï¼šï¼Œã€‚ï¼ˆï¼‰ï¼Ÿï¼ã€Šã€‹ã€ã€‘{}â€œâ€ã€â€”â€”;:,.()?!_]/ /g' test2 | sed 's/[ ][ ]*/ /g' >> test3
```

# VSM

VSMåˆ†ä¸‰æ­¥
1. è¯é•¿åº¦å¯¹é½
2. TF - IDFï¼ˆè€ƒè™‘å¹³æ»‘, similarity æ–¹å¼ï¼‰
3. one by one calaulate

æ€è·¯å¾ˆç®€å•

æˆ‘ä¸€å¼€å§‹è§‰å¾— TF - IDFè®¡ç®—éœ€è¦é’ˆå¯¹æ¯ä¸ª(article1, article2)è¿›è¡Œè®¡ç®—

å› ä¸ºéœ€è¦å¯¹é½ è€Œä¸”æœ€å…³é”®è¦å¹³æ»‘

å¦‚æœæœ‰ä¸ªè¯article1æ²¡æœ‰ï¼Œarticle2ä¹Ÿæ²¡æœ‰ å¦‚ä½•è®¡ç®—tfçš„æ—¶å€™å› ä¸ºè¿›è¡Œå¹³æ»‘å¤„ç† å°±ä¼šå ä¸€å®šæ¯”ä¾‹ è¿™å¯¹äºé‚£äº›é«˜è¯é¢‘çš„ wordå°±ä¸å¤ªå‹å¥½

äºæ˜¯æˆ‘ç¬¬ä¸€ç‰ˆ å°± ä¸€ä¸ªä¸ªéå†è¿‡å» 3100âœ–ï¸3100 ï¼ˆè§`VSM.vsmCalculate()`ï¼‰

çœ‹èµ·æ¥ æ²¡å•¥ ä¹˜èµ·æ¥å°±æ˜¯500w

åˆå§‹åŒ–æ•°ç»„å°±è¦1min

äºæ˜¯å¼€äº†ä¸¤çº§å¤šçº¿ç¨‹

1. æ¯è¡Œä¸ºä¸€ä¸ªçº¿ç¨‹
2. æ¯è¡Œé‡Œé¢æ¯ç»„similarityè®¡ç®—ä¸ºä¸€ä¸ªçº¿ç¨‹

ä½†æ•ˆæœå¾ˆå·® å› ä¸ºé‚£ä¹ˆå¤šä¸ªçº¿ç¨‹äº‰å¤ºå†™ä¸€ä¸ª3100âœ–ï¸3100çš„numpy.Array å‡ºç°äº†å†™é˜»å¡ç°è±¡

é€šè¿‡Activity Monitorè§‚å¯Ÿ å®é™…ä¸Šçº¿ç¨‹æ•°åªæœ‰5.6å·¦å³

æŠŠnumpy.Arrayæ¢æˆlist å‘ç°æ•ˆç‡é«˜äº†ä¸€ç‚¹ è¿˜æ˜¯ä¸è¡Œ

äºæ˜¯ æƒ³èƒ½ä¸èƒ½ä¸å†™ä¸€ä¸ªlist ç›´æ¥ä¸€è¡Œä¸€æ¬¡ å†™æ–‡ä»¶

å‘ç°æ•ˆç‡æé«˜å¾ˆå¤š åŸºæœ¬ä¸Š1sèƒ½å¤„ç†500ä¸ªæ•°æ®

é‚£500wéœ€è¦3h+

äºæ˜¯ç‰ºç‰²ä¸€ä¸‹ç²¾ç¡®åº¦ å…ˆæŒ‰è¯è¢‹é‡Œæ‰€æœ‰è¯ å¯¹é½è¯å‘é‡ï¼ˆè§`VSM.vsmTest()`ï¼‰

å¦‚ä½•å…ˆç”Ÿæˆ3100ç¯‡æ–‡ç« çš„è¯å‘é‡ç»„ï¼ˆtf - idfä¹‹åï¼‰

å†åšä¸€æ¬¡ A.dot(A.T)å°±å¯ä»¥å¾—åˆ°ç»“æœ

å®é™…æ•ˆæœæ€»è€—æ—¶215s çº¦3min æ•ˆæœè¾ƒå¥½

![å›¾ç‰‡.png | center | 556x500](https://cdn.nlark.com/yuque/0/2018/png/104214/1542042196015-6410bc69-0571-4ed1-a7ff-c5459d4240c6.png "")

![å›¾ç‰‡.png | center | 556x500](https://cdn.nlark.com/yuque/0/2018/png/104214/1542042273408-e58c733c-1094-4579-a1d8-6d6a63aaad3e.png "")

å…³äºéšè¯­ä¹‰ç­‰è€ƒå®Œè¯•å†æ¥å¼„

ä¹‹å‰å†™è¿‡ä¸€ç¯‡å…³äºæ—¶åºåˆ†æç›¸å…³å†…å®¹[RNNå®¶æ—çš„blog](/other/rnn.md)

```py
# -*- coding: utf-8 -*-
# @Author: gunjianpan
# @Date:   2018-11-11 20:27:41
# @Last Modified by:   gunjianpan
# @Last Modified time: 2018-11-13 01:07:16

import math
import numpy as np
import pandas as pd
import threading
import time


class VSM():
  """
  handle write vsm ğŸ™‰
  """

  def __init__(self):
    self.articleMaps = []
    self.articleNum = 0
    self.process = 0
    self.resultArray = []
    self.wordMaps = {}
    self.preData()

  def preData(self):
    """
    data prepare
    """
    begin_time()
    file_d = open('test3', 'r')
    articles = file_d.readlines()
    threadings = []
    self.articleNum = len(articles)
    self.articleMaps = [None for i in range(self.articleNum)]
    self.resultArray = [None for i in range(self.articleNum)]
    for index in range(self.articleNum):
      work = threading.Thread(target=self.preDataBasic, args=(
          articles[index].strip('\n').rstrip(), index,))
      threadings.append(work)
    for work in threadings:
      work.start()
    for work in threadings:
      work.join()
    end_time()

  def preDataBasic(self, article, articleId):
    """
    prepare data basic in Threading
    @param article: article string
    @param articleId: article id
    """
    words = article.split(' ')
    wordMap = {}
    for word in words:
      if word in wordMap:
        wordMap[word] = wordMap[word] + 1
      else:
        wordMap[word] = 1
    for word in wordMap:
      if word in self.wordMaps:
        self.wordMaps[word] = self.wordMaps[word] + 1
      else:
        self.wordMaps[word] = 1
    self.articleMaps[articleId] = wordMap

  def tfidfTest(self, wordMap):
    """
    calculate tdidf value
    td use Augmented Frequency 0.5 + 0.5 * fre/maxFre
    """

    wordlist = [wordMap[i] for i in [*wordMap]]
    maxFrequency = max(wordlist)
    tf = np.array([0.5 + 0.5 * index / maxFrequency for index in wordlist])
    idf = np.array([math.log(self.articleNum / self.wordMaps[word])
                    for word in [*wordMap]])
    tfidf = tf * idf
    return tfidf

  def tfidf(self, wordMap):
    """
    calculate tdidf value
    td use Augmented Frequency 0.5 + 0.5 * fre/maxFre
    """

    wordlist = [wordMap[i] for i in [*wordMap]]
    maxFrequency = max(wordlist)
    tf = np.array([0.5 + 0.5 * index / maxFrequency for index in wordlist])
    idf = np.array([math.log(self.articleNum / (1 + self.wordMaps[word]))
                    for word in [*wordMap]])
    tfidf = tf * idf
    return tfidf / np.linalg.norm(tfidf, ord=2)

  def preSimilarity(self, wordMap, index):
    """
    align map and then calculate one tfidf
    """
    tempMap = {
        index: wordMap[index] if index in wordMap else 0 for index in self.wordMaps}
    preMap = {**wordMap, **tempMap}
    self.resultArray[index] = self.tfidf(preMap)
    self.process += 1
    if not self.process % 100:
      print(self.process)

  def vsmTest(self):
    """
    once to calaulate vsm
    """
    begin_time()
    threadings = []
    for index in range(self.articleNum):
      work = threading.Thread(target=self.preSimilarity, args=(
          self.articleMaps[index], index,))
      threadings.append(work)
    for work in threadings:
      work.start()
    for work in threadings:
      work.join()
    tempMatrix = np.array(self.resultArray)
    result = tempMatrix.dot(tempMatrix.T)
    df = pd.DataFrame(result)
    df.to_csv("vsm1.csv", header=False)
    end_time()

  def preSimilarityTest(self, wordMap1, wordMap2):
    """
    align map and then calculate one tfidf
    """
    tempMap1 = {
        index: wordMap1[index] if index in wordMap1 else 0 for index in wordMap2}
    preMap1 = {**wordMap1, **tempMap1}
    return self.tfidfTest(preMap1)

  def similarity(self, wordMap1, wordMap2, types):
    """
    calculate similarity by cos distance
    @Param types: distance calculate type
                =0 Cos Distance
                =1 Chebyshev Distance
                =2 Manhattan Distance
                =3 Euclidean Distance
    """
    tfidf1 = self.preSimilarityTest(wordMap1, wordMap2)
    tfidf2 = self.preSimilarityTest(wordMap2, wordMap1)
    if not types:
      return np.dot(tfidf1, tfidf2) / (np.linalg.norm(tfidf1, ord=2) * np.linalg.norm(tfidf2, ord=2))
    elif types == 1:
      return np.abs(tfidf1 - tfidf2).max()
    elif types == 2:
      return np.sum(np.abs(tfidf1 - tfidf2))
    elif types == 3:
      return np.linalg.norm(tfidf1 - tfidf2)
    else:
      return np.shape(np.nonzero(tfidf1 - tfidf2)[0])[0]

  def vsmCalculate(self):
    """
    calculate vsm
    """
    #: todo write block
    begin_time()
    threadings = []
    for index1 in range(self.articleNum):
      work = threading.Thread(target=self.vsmThread, args=(index1,))
      threadings.append(work)
    for work in threadings:
      work.start()
    for work in threadings:
      work.join()
    end_time()

  def vsmThread(self, index1):
    """
    vsm threading
    """
    nowarticle = self.articleMaps[index1]
    tempResult = []
    for index2 in range(index1, self.articleNum):
      tempResult.append(self.vsmPre(
          nowarticle, self.articleMaps[index2]))

    df = pd.DataFrame({index1: tempResult})
    df.to_csv('vsm.csv', mode='a', header=False)

  def vsmPre(self, wordMap1, wordMap2):
    """
    load data to result
    prevent read block
    """

    self.process += 1
    if not self.process % 100:
      print(self.process)
    return self.similarity(wordMap1, wordMap2, 0)


start = 0


def begin_time():
  global start
  start = time.time()


def end_time():
  print(time.time() - start)
```

ç¥å¤§å®¶è€ƒè¯•é¡ºåˆ©ğŸ™
