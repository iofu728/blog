---
title: ã€HDFSã€ä¼ªåˆ†å¸ƒå¼Hadoopé›†ç¾¤
date: 2018-10-07 15:43:00
tags: [hadoop]
description: hadoop ä¼ªåˆ†å¸ƒå¼
---

æœ¬æ–‡æ˜¯[ã€Hadoopã€MapReduce å¤„ç† æ—¥å¿—log(å•æœºç‰ˆ)](/other/mapreduce.md)çš„æ—­æ–‡, maybeè¿˜æœ‰åç»­

åœ¨æ­å»ºç¯å¢ƒçš„æ—¶å€™å‘ç°å¾ˆéš¾æœåˆ°åˆé€‚çš„æ•™ç¨‹ï¼Œæ‰€ä»¥è¿™ç¯‡åº”è¯¥ä¼šæœ‰ä¸€å®šå—ä¼—

ä¼ªåˆ†å¸ƒå¼å°±æ˜¯**å‡åˆ†å¸ƒå¼**ï¼Œå‡åœ¨å“ªé‡Œï¼Œå‡å°±å‡åœ¨ä»–åªæœ‰ä¸€å°æœºå™¨è€Œä¸æ˜¯å¤šå°æœºå™¨æ¥å®Œæˆä¸€ä¸ªä»»åŠ¡,
ä½†æ˜¯ä»–æ¨¡æ‹Ÿäº†åˆ†å¸ƒå¼çš„è¿™ä¸ªè¿‡ç¨‹ï¼Œæ‰€ä»¥ä¼ªåˆ†å¸ƒå¼ä¸‹Hadoopä¹Ÿå°±æ˜¯ä½ åœ¨ä¸€ä¸ªæœºå™¨ä¸Šé…ç½®äº†hadoopçš„æ‰€æœ‰èŠ‚ç‚¹

ä½†ä¼ªåˆ†å¸ƒå¼å®Œæˆäº†æ‰€æœ‰åˆ†å¸ƒå¼æ‰€å¿…é¡»çš„äº‹ä»¶

ä¼ªåˆ†å¸ƒå¼Hadoopå’Œå•æœºç‰ˆæœ€å¤§çš„åŒºåˆ«å°±åœ¨äºéœ€è¦é…ç½®**HDFS**

## HDFS
`HDFS = Hadoop Distributed File System`

å½“æ•°æ®é‡è¶…è¿‡å•ä¸ªç‰©ç†æœºå™¨ä¸Šå­˜å‚¨çš„å®¹é‡ï¼Œç®¡ç†è·¨è¶Šæœºå™¨çš„ç½‘ç»œå­˜å‚¨ç‰¹å®šæ“ä½œç³»ç»Ÿè¢«ç§°ä¸ºåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼ŒHDFSå°±æ˜¯è¿™æ ·ä¸€ç§ç³»ç»Ÿ

HDFSé›†ç¾¤ä¸»è¦ç”± NameNode ç®¡ç†æ–‡ä»¶ç³»ç»Ÿ Metadata å’Œä»æœº DataNodes å­˜å‚¨çš„å®é™…æ•°æ®

NameNode = Hadoop Masterï¼Œ DataNodes = Hadoop Slave

* NameNode: NameNodeå³ç³»ç»Ÿçš„ä¸»ç«™
  - å…¶ç»´æŠ¤æ‰€æœ‰ç³»ç»Ÿä¸­å­˜åœ¨çš„æ–‡ä»¶å’Œç›®å½•çš„æ–‡ä»¶ç³»ç»Ÿæ ‘å’Œå…ƒæ•°æ®
* DataNode : DataNodesä½œä¸ºä»æœºï¼Œæ¯å°æœºå™¨ä½äºä¸€ä¸ªé›†ç¾¤ä¸­ï¼Œå¹¶æä¾›å®é™…çš„å­˜å‚¨ï¼Œå®ƒè´Ÿè´£ä¸ºå®¢æˆ·è¯»å†™è¯·æ±‚æœåŠ¡

HDFSä¸­çš„è¯»/å†™æ“ä½œè¿è¡Œåœ¨ä¸€ä¸ªä¸ªå—ä¸­ï¼Œå—ä½œä¸ºç‹¬ç«‹çš„å•å…ƒå­˜å‚¨

ä¹‹å‰æˆ‘ä»¬æåˆ°è¿™äº›å—éƒ½æ˜¯é€»è¾‘ä¸Šåˆ’åˆ†çš„ï¼Œåªæ˜¯ç”¨ä¸€ä¸ªç´¢å¼•è®°å½•å—çš„å§‹æœ«ï¼Œå¹¶æœªçœŸæ­£çš„åˆ’åˆ†å—

åœ¨Hadoop 1.xä¸­blocké»˜è®¤size=64Mï¼Œè€ŒHadoop 2.xä¸­é»˜è®¤sizeæ”¹ä¸º128M

å¦‚æœæƒ³é‡æ–°å®šä¹‰`block.size`
```vim
$ vim etc/hadoop/hdfs-site.xml

# add
    <property>
        <name>dfs.block.size</name>
        <value>20971520</value> # å…¶ä¸­è¯¥å€¼ä¸ºå­—èŠ‚Bæ•°ï¼Œä¸”éœ€è¦æ»¡è¶³>1M=1048576=1024Ã—1024, ä¸”èƒ½è¢«512æ•´é™¤
    </property>
```

HDFSæ˜¯å¯å®¹é”™çš„ï¼Œå¯ä¼¸ç¼©çš„ï¼Œæ˜“äºæ‰©å±•ï¼Œé«˜å¯ç”¨çš„

## HDFSè¯»æ“ä½œ

HDFSè¿™ä¸ªç³»ç»Ÿé™¤äº†ä¸»æœºå’Œä»æœºä¹‹å¤–ï¼Œè¿˜åŒ…æ‹¬clientç«¯ï¼Œfile system

![å›¾ç‰‡.png | center | 556x200](https://cdn.nlark.com/yuque/0/2018/jpeg/104214/1538918814787-4ea53425-95fb-4d44-a824-b3f9235e113f.jpeg "")

å½“å®¢æˆ·ç«¯æƒ³è¯»å–ä¸€ä¸ªæ–‡ä»¶çš„æ—¶å€™ï¼Œå®¢æˆ·ç«¯éœ€è¦å’ŒNameNodeèŠ‚ç‚¹è¿›è¡Œäº¤äº’ï¼Œå› ä¸ºå®ƒæ˜¯å”¯ä¸€å­˜å‚¨æ•°æ®èŠ‚ç‚¹å…ƒæ•°æ®çš„èŠ‚ç‚¹

NameNodeè§„å®šå¥´éš¶èŠ‚ç‚¹çš„å­˜å‚¨æ•°æ®çš„åœ°å€è·Ÿä½ç½®

å®¢æˆ·ç«¯é€šè¿‡NameNodeæ‰¾åˆ°å®ƒéœ€è¦æ•°æ®çš„èŠ‚ç‚¹ï¼Œç„¶åç›´æ¥åœ¨æ‰¾åˆ°DataNodeä¸­è¿›è¡Œè¯»æ“ä½œ

è€ƒè™‘åˆ°å®‰å…¨å’Œæˆæƒçš„ç›®çš„ï¼ŒNameNodeç»™å®¢æˆ·ç«¯æä¾›tokenï¼Œè¿™ä¸ªtokenéœ€è¦å‡ºç¤ºç»™DateNoteè¿›è¡Œè®¤è¯ï¼Œè®¤è¯é€šè¿‡åï¼Œæ‰å¯ä»¥è¯»å–æ–‡ä»¶

ç”±äºHDFSè¿›è¡Œè¯»æ“ä½œçš„æ—¶å€™éœ€è¦éœ€è¦è®¿é—®NameNodeèŠ‚ç‚¹ï¼Œæ‰€ä»¥å®¢æˆ·ç«¯éœ€è¦å…ˆé€šè¿‡æ¥å£å‘é€ä¸€ä¸ªè¯·æ±‚ï¼Œç„¶åNameNodeèŠ‚ç‚¹åœ¨è¿”å›ä¸€ä¸ªæ–‡ä»¶ä½ç½®

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼ŒNameNodeè´Ÿè´£æ£€æŸ¥ï¼Œè¯¥å®¢æˆ·ç«¯æ˜¯å¦æœ‰è¶³å¤Ÿçš„æƒé™å»è®¿é—®è¿™ç»„æ•°æ®ï¼Ÿ

å¦‚æœæ‹¥æœ‰æƒé™ï¼ŒNameNodeä¼šå°†æ–‡ä»¶å‚¨å­˜è·¯å¾„åˆ†äº«ç»™è¯¥å®¢æˆ·ç«¯ï¼Œä¸æ­¤åŒæ—¶ï¼Œnamenodeä¹Ÿä¼šå°†ç”¨äºæƒé™æ£€æŸ¥çš„tokenåˆ†äº«ç»™å®¢æˆ·ç«¯

å½“è¯¥å®¢æˆ·ç«¯å»æ•°æ®èŠ‚ç‚¹è¯»æ–‡ä»¶çš„æ—¶å€™ï¼Œåœ¨æ£€æŸ¥tokenä¹‹åï¼Œæ•°æ®èŠ‚ç‚¹å…è®¸å®¢æˆ·ç«¯è¯»ç‰¹å®šçš„block. ä¸€ä¸ªå®¢æˆ·ç«¯æ‰“å¼€ä¸€ä¸ªè¾“å…¥æµå¼€å§‹ä»DataNodeè¯»å–æ•°æ®ï¼Œç„¶åï¼Œå®¢æˆ·ç«¯ç›´æ¥ä»æ•°æ®èŠ‚ç‚¹è¯»å–æ•°æ®

å¦‚æœåœ¨è¯»å–æ•°æ®æœŸé—´datanodesçªç„¶åºŸäº†ï¼Œè¿™ä¸ªå®¢æˆ·ç«¯ä¼šç»§ç»­è®¿é—®Namenode, è¿™æ˜¯NameNodeä¼šç»™å‡ºè¿™ä¸ªæ•°æ®èŠ‚ç‚¹å‰¯æœ¬çš„æ‰€åœ¨ä½ç½®

## HDFSå†™æ“ä½œ

![å›¾ç‰‡.png | center | 556x200](https://cdn.nlark.com/yuque/0/2018/png/104214/1538919255500-ac260c8b-73c3-4496-afab-d7f4c024d858.png "")

æˆ‘ä»¬çŸ¥é“è¯»å–æ–‡ä»¶éœ€è¦å®¢æˆ·ç«¯è®¿é—®Namenode. ç›¸ä¼¼çš„å†™æ–‡ä»¶ä¹Ÿéœ€è¦å®¢æˆ·ç«¯ä¸NameNodeè¿›è¡Œäº¤äº’ï¼ŒNameNodeéœ€è¦æä¾›å¯ä¾›å†™å…¥æ•°æ®çš„å¥´éš¶èŠ‚ç‚¹çš„åœ°å€

å½“å®¢æˆ·ç«¯å®Œæˆåœ¨blockä¸­å†™æ•°æ®çš„æ“ä½œæ—¶ï¼Œè¿™ä¸ªå¥´éš¶èŠ‚ç‚¹å¼€å§‹å¤åˆ¶è‡ªèº«ç»™å…¶ä»–å¥´éš¶èŠ‚ç‚¹ï¼Œç›´åˆ°å®Œæˆæ‹¥æœ‰nä¸ªå‰¯æœ¬(è¿™é‡Œçš„nä¸ºå‰¯æœ¬å› å­æ•°)

å½“å¤åˆ¶å®Œæˆåï¼Œå®ƒä¼šç»™å®¢æˆ·ç«¯å‘ä¸€ä¸ªé€šçŸ¥ï¼ŒåŒæ ·çš„è¿™ä¸ªæˆæƒæ­¥éª¤ä¹Ÿå’Œè¯»å–æ•°æ®æ—¶ä¸€æ ·

å½“ä¸€ä¸ªå®¢æˆ·ç«¯éœ€è¦å†™å…¥æ•°æ®çš„æ—¶å€™ï¼Œå®ƒéœ€è¦è·ŸNameNodeè¿›è¡Œäº¤äº’ï¼Œå®¢æˆ·ç«¯å‘é€ä¸€ä¸ªè¯·æ±‚ç»™NameNode, NameNodeåŒæ—¶è¿”å›ä¸€ä¸ªå¯å†™çš„åœ°å€ç»™å®¢æˆ·ç«¯

ç„¶åå®¢æˆ·ç«¯ä¸ç‰¹å®šçš„DataNodeè¿›è¡Œäº¤äº’ï¼Œå°†æ•°æ®ç›´æ¥å†™å…¥è¿›å»ã€‚å½“æ•°æ®è¢«å†™å…¥å’Œè¢«å¤åˆ¶çš„è¿‡ç¨‹å®Œæˆåï¼ŒDatanodeå‘é€ç»™å®¢æˆ·ç«¯ä¸€ä¸ªé€šçŸ¥ï¼Œå‘ŠçŸ¥æ•°æ®å†™å…¥å·²ç»å®Œæˆ

å½“å®¢æˆ·ç«¯å®Œæˆå†™å…¥ç¬¬ä¸€ä¸ªblockæ—¶ï¼Œç¬¬ä¸€ä¸ªæ•°æ®èŠ‚ç‚¹ä¼šå¤åˆ¶ä¸€æ ·çš„blockç»™å¦ä¸€ä¸ªDataNode, ç„¶ååœ¨è¿™ä¸ªæ•°æ®èŠ‚ç‚¹å®Œæˆæ¥æ”¶blockä¹‹åï¼Œå®ƒå¼€å§‹å¤åˆ¶è¿™äº›blocksç»™ç¬¬ä¸‰ä¸ªæ•°æ®èŠ‚ç‚¹

ç¬¬ä¸‰ä¸ªæ•°æ®èŠ‚ç‚¹å‘é€é€šçŸ¥ç»™ç¬¬äºŒä¸ªæ•°æ®èŠ‚ç‚¹ï¼Œç¬¬äºŒä¸ªæ•°æ®èŠ‚ç‚¹åœ¨å‘é€é€šçŸ¥ç»™ç¬¬ä¸€ä¸ªæ•°æ®èŠ‚ç‚¹ï¼Œç¬¬ä¸€ä¸ªæ•°æ®èŠ‚ç‚¹è´Ÿè´£æœ€åé€šçŸ¥å®¢æˆ·ç«¯

ä¸è®ºå‰¯æœ¬å› å­æ˜¯å¤šå°‘ï¼Œå®¢æˆ·ç«¯åªå‘é€ä¸€ä¸ªæ•°æ®å‰¯æœ¬ç»™DataNode, DataNodeå®Œæˆåç»­æ‰€æœ‰ä»»åŠ¡çš„å¤åˆ¶å·¥ä½œ

æ‰€ä»¥ï¼Œåœ¨Hadoopä¸­å†™å…¥æ–‡ä»¶å¹¶ä¸ååˆ†æ¶ˆè€—ç³»ç»Ÿèµ„æºï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨å¤šä¸ªæ•°æ®ç‚¹å°†blockså¹³è¡Œå†™å…¥

## HDFS é…ç½®

```vim
# ä¿®æ”¹ä¸ºä¸»æœºå
$ vim etc/hadoop/slaves
# æ·»åŠ ä¸»æœºå
$ vim /etc/hosts
```
```vim
# create folder
$ mkdir -p /usr/local/hadoop/tmp
# ä¿®æ”¹tempåœ°å€
$ vim etc/hadoop/core-site.xml

    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value> ## localhostå¡«ä¸»æœºå->ä¸»æœºåä¹Ÿæ˜¯ä¸€ä¸ªå‘
    </property>
```
```vim
# create folder
$ mkdir -p /usr/local/hadoop/dfs/name
$ mkdir -p /usr/local/hadoop/dfs/data
# ä¿®æ”¹åˆ†å¸ƒå¼å‚¨å­˜é…ç½®
$ vim etc/hadoop/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>10</value> ## nodeæ•°
    </property>
    <property>
        <name>dfs.block.size</name>
        <value>20971520</value> ## block.size
    </property>
   <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/dfs/name</value>
   </property>
   <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/dfs/data</value>
   </property>
   <property>
    <name>dfs.permissions</name>
    <value>false</value>
   </property>
</configuration>
```
```vim
# yarn é…ç½®ï¼Œæ³¨æ„æ‰€æœ‰localhostä¸ºä¸»æœºåï¼Œéœ€ä¿æŒä¸€è‡´
$ vim etc/hadoop/yarn-site.xml
<configuration>
   <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>localhost:8033</value>
   </property>
   <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
   </property>
   <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
   </property>
   <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>localhost:8025</value>
   </property>
   <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>localhost:8030</value>
   </property>
   <property>
    <name>yarn.resourcemanager.address</name>
    <value>localhost:8050</value>
   </property>
   <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>localhost:8030</value>
   </property>
   <property>
     <name>yarn.resourcemanager.webapp.address</name>
     <value>localhost:8088</value>
   </property>
   <property>
    <name>yarn.resourcemanager.webapp.https.address</name>
    <value>localhost:8090</value>
   </property>
</configuration>
```
å®Œæˆä¸Šè¿°é…ç½®ä¹‹åï¼Œéœ€è¦å¯¹FSè¿›è¡Œç›¸åº”çš„æ ¼å¼åŒ–æ“ä½œ

```vim
hdfs namenode -format
hdfs getconf -namenodes
```

ç„¶åå°±å¯ä»¥å¯åŠ¨hdfsäº†

```vim
$ bash sbin/start-all.sh
```

å¯åŠ¨ä¹‹åå¯ä»¥é€šè¿‡Jbså‘½ä»¤æŸ¥çœ‹è¿›ç¨‹

ä¹Ÿå¯ä»¥é€šè¿‡http://ip:50070è¿›å…¥HDFSçš„å‰ç«¯è¿›è¡Œæ–‡ä»¶ç®¡ç†


## æŠ¥é”™FAQ
1. Hadoop 3.1.1ç‰ˆæœ¬åœ¨bash sbin/start-dfs.shä¼šæŠ¥[bash v3.2+ is required. Sorry.](https://segmentfault.com/q/1010000015478820)

  è¿™ä¸ªæ²¡æ³•è§£å†³ï¼Œæœäº†å¥½å¤šéƒ½æ²¡æœ‰ç­”æ¡ˆï¼ŒæŠŠåŒ…æ¢æˆ2.9.1å°±å¥½äº†
  PS: è®°å¾—æŠŠæ‰€æœ‰ç¯å¢ƒå˜é‡ï¼Œè½¯é“¾æ¥éƒ½æ”¹ä¸€é

2. [bash sbin/start-dfs.shæŠ¥](https://stackoverflow.com/questions/48129029/hdfs-namenode-user-hdfs-datanode-user-hdfs-secondarynamenode-user-not-defined)
```vim
Starting namenodes on [localhost]
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
```
é…ç½®ç¯å¢ƒå˜é‡
```vim
$ vim ~/.zshrc
export HDFS_NAMENODE_USER="root"
export HDFS_DATANODE_USER="root"
export HDFS_SECONDARYNAMENODE_USER="root"
export YARN_RESOURCEMANAGER_USER="root"
export YARN_NODEMANAGER_USER="root"
$ source ~/.zshrc
```
3. [localhost: Error: JAVA_HOME is not set.](https://stackoverflow.com/questions/14325594/working-with-hadoop-localhost-error-java-home-is-not-set)

```vim
$ vim etc/hadoop/hadoop-env.sh
export JAVA_HOME=/etc/local/java/xxx ## ç»å¯¹è·¯å¾„
```

4. [command not found](https://stackoverflow.com/questions/38030730/start-all-sh-start-dfs-sh-command-not-found)

```vim
$ vim ~/.zshrc
PATH=$PATH:/usr/local/hadoop/sbin
$ source ~/.zshrc
```

5. [Incorrect configuration: namenode address dfs.namenode.rpc-address is not configured](https://stackoverflow.com/questions/23719912/incorrect-configuration-namenode-address-dfs-namenode-rpc-address-is-not-config)

```vim
$ export HADOOP_CONF_DIR = $HADOOP_HOME/etc/hadoop
$ echo $HADOOP_CONF_DIR
$ hdfs namenode -format
$ hdfs getconf -namenodes
$ etc/hadoop/start-all.sh
```

6. [adoop Mapreduce Error Input path does not exist: hdfs://localhost:9000/user/log/input"](https://stackoverflow.com/questions/32179761/hadoop-mapreduce-error-input-path-does-not-exist-hdfs-localhost54310-user-hd)

hdfsç³»ç»Ÿå’Œå¤–éƒ¨æ–‡ä»¶ç³»ç»Ÿä¸åŒæ­¥ï¼Œéœ€è¦æ‰‹åŠ¨æŠŠæ–‡ä»¶ä¼ è¿›å», hdfsæœ‰ä¸€å¥—ç±»ä¼¼äºå¤–éƒ¨æ–‡ä»¶ç³»ç»Ÿçš„fså‘½ä»¤
```vim
$ hadoop fs -mkdir -p /user/log/input
$ hadoop fs -put <datafile>  /user/log/input
```

æ–‡ä»¶ä¹Ÿå¯ä»¥åœ¨http://ip:50070ä¸­æŸ¥çœ‹

![å›¾ç‰‡.png | center | 556x200](https://cdn.nlark.com/yuque/0/2018/png/104214/1538923613086-7b258284-0dc9-4d90-9482-a9a9bda8ab1b.png "")

7. [hadoop fs å‘½ä»¤](https://segmentfault.com/a/1190000002672666)

```vim
$ hadoop fs -ls  /
$ hadoop fs -put < local file > < hdfs file >
$ hadoop fs -moveFromLocal  < local src > ... < hdfs dst >
$ hadoop fs -copyFromLocal  < local src > ... < hdfs dst >
$ hadoop fs -get < hdfs file > < local file or dir>
```

## æ€§èƒ½å¯¹æ¯”

å•æœºç‰ˆ
```vim
    Reduce input groups=1
        Reduce shuffle bytes=14
        Reduce input records=1
        Reduce output records=1
        Spilled Records=2
        Shuffled Maps =1
        Failed Shuffles=0
        Merged Map outputs=1
        GC time elapsed (ms)=33
        Total committed heap usage (bytes)=291282944
    Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
    File Input Format Counters
        Bytes Read=2977
    File Output Format Counters
        Bytes Written=18
spend:21559ms
```
ä¼ªåˆ†å¸ƒ-1èŠ‚ç‚¹ï¼Œblock.size=10M
```vim
        Map input records=180
        Map output records=180
        Map output bytes=1080
        Map output materialized bytes=14
        Input split bytes=106
        Combine input records=180
        Combine output records=1
        Reduce input groups=1
        Reduce shuffle bytes=14
        Reduce input records=1
        Reduce output records=1
        Spilled Records=2
        Shuffled Maps =1
        Failed Shuffles=0
        Merged Map outputs=1
        GC time elapsed (ms)=31
        Total committed heap usage (bytes)=287883264
    Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
    File Input Format Counters
        Bytes Read=2941
    File Output Format Counters
        Bytes Written=6
spend:23277ms
```
ä¼ªåˆ†å¸ƒ-10èŠ‚ç‚¹ï¼Œblock.size=20M
```vim
ted successfully
18/10/07 22:51:57 INFO mapreduce.Job: Counters: 35
    File System Counters
        FILE: Number of bytes read=683924
        FILE: Number of bytes written=8139284
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=281872252
        HDFS: Number of bytes written=181666
        HDFS: Number of read operations=181
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=88
    Map-Reduce Framework
        Map input records=180
        Map output records=180
        Map output bytes=1080
        Map output materialized bytes=14
        Input split bytes=106
        Combine input records=180
        Combine output records=1
        Reduce input groups=1
        Reduce shuffle bytes=14
        Reduce input records=1
        Reduce output records=1
        Spilled Records=2
        Shuffled Maps =1
        Failed Shuffles=0
        Merged Map outputs=1
        GC time elapsed (ms)=39
        Total committed heap usage (bytes)=287825920
    Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
    File Input Format Counters
        Bytes Read=2941
    File Output Format Counters
        Bytes Written=6
spend:24229ms
```

æ€»çš„æ¥è¯´å› ä¸ºåœ¨ä¸€å°æœºå­ä¸Š, ä¼ªåˆ†å¸ƒæ€§èƒ½å¹¶æ²¡æœ‰æå‡

ä½†è·‘èµ·æ¥ æˆ‘é‚£å°ç ´æœåŠ¡å™¨ å†…å­˜å°±è·Œé›¶äº† ææ€–ğŸ˜±

![å›¾ç‰‡.png | center | 556x200](https://cdn.nlark.com/yuque/0/2018/png/104214/1538926799174-cc52d4c4-49d6-47d6-8de0-9b171ef81684.png "")

![å›¾ç‰‡.png | center | 556x200](https://cdn.nlark.com/yuque/0/2018/png/104214/1538926853109-db4dc1ff-6d3c-4c1d-97bb-b26ea818b696.png "")

ä¸å……é’±æ€ä¹ˆå˜å¾—å¼ºå¤§

## å‚è€ƒ

1. [Hadoop HDFSå…¥é—¨](https://www.yiibai.com/hadoop/hdfs_beginners_guide.html)
2. [Hadoop HDFS æ•™ç¨‹ï¼ˆä¸€ï¼‰ä»‹ç»](https://www.jianshu.com/p/8969eb90a59d)
3. [Hadoopä¼ªåˆ†å¸ƒå¼é›†ç¾¤æ­å»º](https://www.jianshu.com/p/1352ce8c8d73)





