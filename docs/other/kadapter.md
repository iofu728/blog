---
title: é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å¯æ’æ‹”å¼çŸ¥è¯†èå…¥-åˆ©ç”¨Adapterç»“æ„
date: 2020-03-08 03:06:06
tags: [NLP, Knowledge]
description: Injecting Knowledge into Language Models using Adapter
---

> è¿™äº›å¤©å†²æµª ğŸ„ äº†ä¸€ä¸‹ Arxivï¼Œç²—ç¿»äº†ä¸€ä¸‹äºŒæœˆä¹‹å public çš„ NLP æ–‡ç« (æœ‰å¾ˆå¤§ä¸€éƒ¨åˆ†æ˜¯å‡†å¤‡æŠ• ICML çš„)ã€‚  
> ä¹Ÿæ‹œè¯»äº† å¼ é©°åŸ dalao çš„æ–°ä½œã€ŠExploring the Memorization-Generalization Continuum in Deep Learningã€‹. (å®éªŒçœŸçš„åšçš„å¾ˆæ¼‚äº®,ä½†æ„Ÿè§‰æœ‰ç‚¹ data-special ä¸çŸ¥é“èƒ½ä¸èƒ½æ¨å¹¿åˆ° NLP)

ä»Šå¤©æ¥è®¨è®ºä¸€ä¸‹æ®µæ¥ è€å¸ˆå’Œå‘¨æ˜è€å¸ˆçš„è¿™ç¯‡[ã€ŠK-Adapter: Infusing Knowledge into Pre-Trained Models with Adaptersã€‹][1]

## Adapter

ä¸ºäº†è®²æ¸…æ¥šè¿™ç¯‡æ–‡ç« ï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸‹ä»€ä¹ˆæ˜¯ Adapter

[Parameter-Efï¬cient Transfer Learning for NLP.][2] ICML 2019.

### Motivation

åœ¨è¿™ä¸ª large pre-trained æ¨¡å‹ç››è¡Œçš„æ—¶ä»£ï¼ŒFine-tune å¯è°“æ˜¯å†æ­£å¸¸ä¸è¿‡çš„æ“ä½œã€‚  
ä½†å®é™…ä¸Š Fine-tune æ˜¯ä¸€ä¸ªä»£ä»·å¾ˆå¤§çš„æ“ä½œï¼Œè™½ç„¶å®ƒä¸€èˆ¬èƒ½å¸¦æ¥å¾ˆå¥½çš„æ•ˆæœã€‚  
è¯•æƒ³ä¸€ä¸‹ï¼Œè™½ç„¶æˆ‘ä»¬ç”¨äº† Adam æ¥éšæœºé‡‡æ ·ä¸€äº› train data æ¥ä¼°è®¡å…¨å±€çš„æ¢¯åº¦ï¼Œç”¨äº†å¾ˆå°çš„ lr.  
ä½†å®é™…ä¸Šåœ¨æ¯ä¸€ä¸ª batch ä¸­,å¯¹äºåºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹çš„æ¯ä¸€ä¸ªå‚æ•°æˆ‘ä»¬éƒ½éœ€è¦æ›´æ–°.  
æ¯ä¸€ä¸ª epoch, è¿˜å¾—å­˜å‚¨æ‰€æœ‰è¢«æ›´æ–°çš„å‚æ•°, å®Œå…¨æ²¡æœ‰å¤ç”¨æ€§, è¿™æ˜¯å¾ˆä½æ•ˆçš„ã€‚  
å¯¹äºä½èµ„æºçš„ç§»åŠ¨ç«¯æˆ–è€…é«˜ç”¨æˆ·ç‰¹å¼‚æ€§çš„æœåŠ¡æä¾›å•† Pass, è¿™ä¸ªé—®é¢˜å°¤ä¸ºçªå‡ºã€‚

é™¤äº†è¿™ä¸€ç‚¹ä¹‹å¤–:

1. Cloud Service (Pass)
2. ä½¿ç”¨ Multi-task æ¥ fine-tune æ—¶, å¦‚æœå¢åŠ æ–°çš„ä»»åŠ¡ï¼Œåˆ™éœ€è¦é‡æ–°è®­ç»ƒè¿‡æ‰€æœ‰ä¹‹å‰çš„å­ä»»åŠ¡(éœ€è¦ç›¸åº”çš„æ•°æ®).
3. è€Œä½¿ç”¨ è¿ç»­å­¦ä¹ åˆ™ä¼šåœ¨ re-training çš„æ—¶å€™é—å¿˜ä¹‹å‰å­¦åˆ°çš„çŸ¥è¯†.
4. å¸Œæœ›èƒ½åœ¨å°½å¯èƒ½å‡å°‘å‚æ•°çš„æƒ…å†µä¸‹, æé«˜æ€§èƒ½,æ¥è¿‘ Multi-task çš„ç»“æœ.

### Detail

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687915650-b21f5a3f-9316-4208-924f-b19d2d42b800.png)

äºæ˜¯ä¸€ä¸ªå¾ˆç›´è§‚çš„æƒ³æ³•, èƒ½ä¸èƒ½æŠŠæœ€åçš„ task-special layer æ”¾åˆ°æ¨¡å‹ä¸­é—´ï¼Œç„¶åå†»ä½é¢„è®­ç»ƒæ¨¡å‹å‚æ•°.

1. æ¯ä¸€ä¸ª Transformer ç»“æ„éƒ½æœ‰ä¸¤ä¸ª Adapter æ¨¡å—, åµŒåœ¨ LN ä¹‹å‰. 12 Ã— 2
2. é¢„è®­ç»ƒçš„ Bert å‚æ•°å›ºå®š(Attention, FFN, é™¤äº† Layer Normalization å‚æ•°ä¸å›ºå®š)
3. æ¯ä¸ª Adapter ç”±ä¸¤ä¸ª FFN, ä¸€ä¸ªéçº¿æ€§å‡½æ•°ç»„æˆ, å’Œä¸€ä¸ªæ®‹å·®è¿æ¥ç»„æˆ.
4. æ®‹å·®è¿æ¥ç”¨äºä¿è¯å‚æ•°éšæœºåˆå§‹åŒ–æ—¶ï¼Œæ¨¡å‹è¾“å‡ºä¸é¢„è®­ç»ƒæ¨¡å‹è¾“å‡ºä¸€è‡´.
5. è¿™æ ·ä¸€ä¸ª Adapter æ¨¡å‹éœ€è¦ (dm+m) + (dm+d)å‚æ•°
6. è€Œå› ä¸º LN è¾“å…¥å‘ç”Ÿäº†è¾ƒå¤§çš„å˜åŒ–ï¼Œåœ¨è¿™é‡Œå¯¹ LN çš„å‚æ•°ä¹Ÿè¿›è¡Œ fine-tune, å®é™…ä¸Šè¿™éƒ¨åˆ†å‚æ•°é‡å¾ˆå°($y=\frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta$)
7. æ•…æ€»å…±ä¸€å±‚ Transformer éœ€è¦å¢åŠ (2dm+3d+m), è¿™éƒ¨åˆ†ä¸ m æœ‰å…³, ä½†æ€»çš„å‚æ•°é‡å¤§æ¦‚æ˜¯é¢„è®­ç»ƒæ¨¡å‹æ€»å‚æ•°é‡çš„ 3%å·¦å³ã€‚

### Experiments

åœ¨æ¨¡å‹çš„æœ€åä¸€å±‚æ¥ä¸€ä¸ªçº¿æ€§å±‚, åœ¨åˆ†ç±»ä»»åŠ¡ GLUE å’Œä¸€äº›é¢å¤–çš„åˆ†ç±»ä»»åŠ¡ä¸Šæµ‹è¯•ï¼ŒåŸºæœ¬ä¸Šç»“æœå¾ˆæ¥è¿‘ Fine-tune çš„ç»“æœ.

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687916632-d71e72b2-4464-4f06-a686-49180400f39e.png)

å½“ç„¶åªåœ¨æ¯”è¾ƒç®€å•çš„ Classify Task ä¸Šæµ‹è¯•, è¯´æœåŠ›æ²¡æœ‰é‚£ä¹ˆå¼ºã€‚

#### Parameters

æ—¢ç„¶ä¸¤è€…ç»“æœå¾ˆæ¥è¿‘ æ˜¯ä¸æ˜¯ Fine-tune å®é™…ä¸Šå¹¶ä¸éœ€è¦æ›´æ–°é‚£ä¹ˆå¤šå‚æ•°ä¹Ÿèƒ½æœ‰é‚£ä¹ˆå¥½çš„ç»“æœå‘¢ï¼Ÿ

è¿™éƒ¨åˆ†å¯¹æ¯”ä¸¤ä¸ª baseline:

1. åª Fine-tune Top N å±‚ Transformer çš„å‚æ•°.
2. åªæ›´æ–° LN çš„å‚æ•°(Ablation)

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687918228-d6bfedd6-11e4-44b4-b223-f828f0b8026d.png)

1. å½“æˆ‘ä»¬å‡å°‘ Fine-tune å±‚æ•°çš„æ—¶å€™, æ¨¡å‹çš„å‡†ç¡®ç‡æ€¥å‰§ä¸‹é™;
2. è€Œ Adapter åˆ™å…·æœ‰å¾ˆå¥½çš„é²æ£’æ€§.
3. Fine-tune LN å‚æ•°åŸºæœ¬æ²¡ç”¨

#### Does every Adapter layers are significant?

å®é™…ä¸Šï¼Œæˆ‘ä»¬ä¸€å£æ°”ç»™ 24(BERT large)ä¸ª Transformer Layer éƒ½åŠ ä¸Šäº† Adapter, é‚£æ˜¯ä¸æ˜¯æ¯ä¸€ä¸ª Adapter éƒ½å¾ˆé‡è¦ï¼Ÿ

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687919843-7b516431-6189-4064-9274-fe0cec01bbb5.png)

ä¸Šè¿° Ablation å®éªŒç»“æœ,æˆ‘ä»¬å¯ä»¥å‘ç°:

1. å»é™¤å•å±‚ Adapter åŸºæœ¬ä¸Šå¯¹ç»“æœæ²¡æœ‰å½±å“;
2. ä½å±‚ï¼Œå°¤å…¶æ˜¯ 0-4 å±‚å¯¹ç»“æœå½±å“ä¸å¤§;
3. ä½å±‚è•´å«çš„ä¿¡æ¯æ›´å¤šæ˜¯ä»»åŠ¡é€šç”¨çš„ï¼Œè€Œé«˜å±‚æ›´å¤šæ˜¯ä»»åŠ¡ç‰¹æ®Šçš„çŸ¥è¯†;
4. åˆå§‹åŒ–å‚æ•°çš„æ–¹å·®ä¸èƒ½è¿‡å¤§.

é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æµ‹è¯•äº†

1. å¢åŠ  BN/LN
2. å¢åŠ æ¯ä¸ª Adapter çš„å±‚æ•°
3. æ›´æ”¹ä¸åŒçš„æ¿€æ´»å‡½æ•°

ç­‰ç­‰ä¿®æ”¹ï¼Œä½†æ˜¯å‘ç°ç»“æœåŸºæœ¬æ²¡æœ‰å½±å“

### PALs

[BERT and PALs: Projected Attention Layers for Efï¬cient Adaptation in Multi-Task Learning.][3] ICML 2019.

åŒæœŸè¿˜æœ‰ä¸€ç¯‡å·¥ä½œä¹Ÿæ˜¯æƒ³å°½å¯èƒ½å‡å°‘ Fine-tune æ—¶å‚æ•°çš„æ›´æ–°é‡, å…¶å°† Task-special Layer ç§»è‡³ Transformer ä¸¤ä¸ª LN ä¹‹é—´ã€‚

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687921365-9b3a2dc1-0dab-4101-9ee3-f7f13f2e747a.png)

é€šè¿‡å…ˆæŠ•å½±åˆ°åˆ°ä¸€ä¸ªå°ç»´åº¦ï¼Œå†è¿æ¥ Attention æˆ–è€…å…¶ä»–ç»“æ„æ¥å®Œæˆ Fine-tune çš„ä»»åŠ¡.

CS224n 2019 Final Project ä¸­æœ‰ä¸¤ä½åŒå­¦å¯¹ä¸Šè¿°ä¸¤ç§æ–¹æ³•åœ¨ SQuAD 2.0 ä¸Šåšäº†ç›¸åº”çš„æµ‹è¯•, ç»“æœæ˜¾ç¤º PALs ç»“æœæ‰çš„æœ‰ç‚¹å¤š, è€Œ Adapter-BERT ç»“æœå¾ˆæ¥è¿‘ Fine-tune ç»“æœ.

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687923678-ccb105a0-e6b6-4aaf-ac31-ad7673a24c35.png)

## K-Adapter

è€Œè¿™ç¯‡æ–‡ç« æ›´ä¾§é‡äºæ”¹è¿›é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ Multi-task è¿™ä¸ªè¿‡ç¨‹

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687925114-7c896c24-8723-48f9-95e6-692cbe49d187.png)

### Motivations

1. é¢„è®­ç»ƒæ¨¡å‹ä¸­åµŒå…¥çŸ¥è¯†ä¿¡æ¯æ˜¯å¾ˆæœ‰å¿…è¦çš„.
   1. åŸºäºæ— ç›‘ç£å­¦ä¹ çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹æ›´å€¾å‘äºå­¦ä¹ å…±ç°ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†ä½é¢‘ä½†é‡è¦çš„çŸ¥è¯†ä¿¡æ¯ã€‚
   2. åœ¨æ¨ç†ä»»åŠ¡ä¸Šæ•ˆæœè¾ƒå·®, (Not, reasoning task)
2. Multi-task ä¼šé€ æˆçŸ¥è¯†é—å¿˜, è€Œä¸”å‚æ•°è®¡ç®—ä»£ä»·æ˜¯å·¨å¤§çš„
   1. å…ˆå‰çš„ KB-based çš„ pre-trained LM å¤§å¤šæ˜¯åŸºäº multi-task çš„
   2. å½“èåˆå¤šç§çŸ¥è¯†çš„æ—¶å€™ multi-task ä»£ä»·å¤§ï¼Œä¹Ÿå®¹æ˜“é—å¿˜ä¹‹å‰å­¦ä¹ è¿‡çš„ä»»åŠ¡

### Details

æ‰€ä»¥é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ª Adapter-based çš„æ¨¡å‹æ¥è§£å†³ä¸Šè¿°é—®é¢˜.  
é€šè¿‡å¹¶è¡Œçš„ Adapter å±‚æ¥è·å¾—ä¸åŒç±»å‹çš„çŸ¥è¯†ä¿¡æ¯ï¼Œæœ€åé€šè¿‡ concatenate æ¥è¾“å‡ºï¼Œäº’ä¸å½±å“.

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687926871-16a50202-1515-4910-a63e-cdb6ec4b9535.png)

1. ç›¸å¯¹äºå‰é¢çš„ Adapter ç»“æ„, K-Adapter å°† Transformer ç»“æ„ç›´æ¥åµŒå…¥åˆ° Adapter Layer ä¸­ã€‚
2. ä½ç½®ç»“æ„å‘ç”Ÿäº†å˜åŒ–, Adapter-BERT æ˜¯ç›´æ¥æ”¹é€  Transformer ç»“æ„ï¼Œæ¯ä¸ª Transformer Layer éƒ½æœ‰ä¸¤ä¸ª Adapter Layer; è€Œ K-Adapter åˆ™å°† Adapter ç‹¬ç«‹å‡ºæ¥ä¸ Pre-trained model å¹³è¡Œæ“ä½œï¼Œé€šè¿‡ Concatenate ä¼ é€’ä¿¡æ¯, ä¹Ÿä¸æ˜¯æ¯å±‚éƒ½é…æœ‰ Adapter Layer, æœ¬æ–‡ä¸­æ˜¯åœ¨ RoBERTa Large çš„ç¬¬ 0, 11, 23 å±‚ä¹‹åå¢åŠ æœ‰ Adapter å±‚ã€‚
3. éœ€è¦çš„å‚æ•°é‡ 3(FFN + Transformer) = 3(2dm + d +m) + (3m^2 + m^2 + 8m^2 + 2m)) = 47M è¿œå°äº RoBERTa Large æ¨¡å‹ä¸­ 16355M çš„å‚æ•°é‡.
4. ç›¸åŒçš„ skip-connect ä¸ºäº†åˆå§‹åŒ–æ—¶çš„ä¸€è‡´æ€§(Concatenate ä¼ é€’äº† Transformer çš„è¾“å‡º)
5. Concatenate å‰ä¸€ Adapter çš„è¾“å‡ºå’Œå½“å‰å±‚ Transformer çš„è¾“å‡ºä½œä¸ºå½“å‰ Adapter çš„è¾“å…¥. (Concatenate åœ¨è¿™é‡Œä¼šé€ æˆç»´åº¦ä¸ä¸€è‡´ï¼Œæ—¢ç„¶ä¹‹åéƒ½æ˜¯çº¿æ€§å±‚ï¼Œç”¨åŠ ä¹Ÿæ˜¯ç­‰æ•ˆçš„ï¼Œè¿˜èƒ½é™ä½å‚æ•°é‡)
6. å•ä¸ª knowledge task çš„è¾“å‡ºæ˜¯æœ€åä¸€ä¸ª Adapter çš„è¾“å‡ºå’Œæœ€åä¸€ä¸ª Transformer è¾“å‡º Concatenate åœ¨ä¸€èµ·, è®°ä¸º O_k.
7. å½“æœ‰å¤šä¸ª Knowledge ä¸€èµ·èå…¥æ—¶, Concatenate æ¯ä¸ª Knowledge è¾“å‡ºçš„ç»“æœ Concate(O_1, O_2, ...).
8. è¿™ç¯‡æ–‡ç« ä½¿ç”¨äº†ä¸¤ç§ Adapter: äº‹å® Adapter, è¯­è¨€ Adapter
9. äº‹å® Adapter è®­ç»ƒä¸€ä¸ªå…³ç³»åˆ†ç±»ä»»åŠ¡ã€‚é€šè¿‡åˆ¤æ–­ä¸‰å…ƒç»„ä¸­ entity æ˜¯å¦å­˜åœ¨ç›¸åº”å…³ç³»æ¥å­¦ä¹ å…³ç³»çš„çŸ¥è¯†ã€‚æ•°æ®é›†æ˜¯è¿‡æ»¤ entity å‡ºç°å°äº 50 æ¬¡çš„ T-RE-rc. å› ä¸º Entity é•¿åº¦ä¸ä¸€ï¼Œåˆ©ç”¨ Pooling æ¥å¯¹é½. è¯¥ä»»åŠ¡è®­ç»ƒ 5epochs, Batch size ä¸º 128.
10. è¯­è¨€ Adapter åˆ™æ˜¯å®Œæˆé¢„æµ‹ä¾å­˜å…³ç³»ä¸­çˆ¶èŠ‚ç‚¹ index è¿™ä¸ªä»»åŠ¡ã€‚æ•°æ®é›†æ˜¯åˆ©ç”¨ Stanford Parser æ ‡æ³¨çš„ Book Corpusã€‚å› ä¸ºæ˜¯ token-level çš„ä»»åŠ¡ï¼Œæœ€åè¿‡ä¸€ä¸ªçº¿æ€§å±‚è¾“å‡ºåˆ°ç›¸åº”çš„åˆ†ç±»ã€‚è¯¥ä»»åŠ¡è®­ç»ƒ 10epochs, Batch size ä¸º 256

### Baselines

å®éªŒå¯¹æ¯”äº†æœ€è¿‘æå‡ºçš„ä¸€äº›åˆ—å°†çŸ¥è¯†èå…¥é¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•.

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687929342-0b913e67-bcfc-4a9b-9869-9f9718a3aa71.png)

1. ERNIE: å¯¹é½ WikiData ä¸­çš„ä¸‰å…ƒç»„åˆ° Wikipedia çš„å¥å­ä¸­, å°†ç”¨ TransE é¢„è®­ç»ƒçš„ entity ä¿¡æ¯åŠ å…¥åˆ°å¯¹åº”çš„ token ä¸­.
2. LIBERT: å¢åŠ  Lexical Relation Classification(LRC)ä»»åŠ¡ï¼Œåˆ¤æ–­ä¸Šä¸‹è°“è¯.
3. SenseBERT: Mask token ä½¿å…¶é¢„æµ‹ç›¸åº”çš„è¯åŠå…¶å¯¹åº”çš„ supersense(ç±»ä¼¼ä¸€ä¸ª POS å†åŠ ä¸Šç»†ç²’åº¦çš„ entity)
4. KnowBERT: äº¤æ›¿è®­ç»ƒ BERT å’Œ Entity Link ä»»åŠ¡(freeze)
5. WKLM: å°†å®ä½“æ›¿æ¢ä¸º WikiData ä¸­ç›¸åŒç±»å‹çš„å…¶ä»–å®ä½“.
6. BERT-MK: ç»“æ„ä¸ ERNIE ç›¸åŒ, å°† TransE æ›¿æ¢ä¸º GATs.

### Experiment

ç›¸å¯¹äºä¹‹å‰é‚£ç¯‡ Adapter-BERT, è¿™ç¯‡çš„å®éªŒè®¾è®¡æ›´èƒ½è¯´æ˜å­¦ä¹ åˆ°çŸ¥è¯†çš„èƒ½åŠ›ï¼Œå®éªŒç»“æœä¹Ÿå¥½äºä¹‹å‰çš„å·¥ä½œ(åŸå…ˆåªæ˜¯æƒ³æ¥è¿‘ Fine-tune, ç°åœ¨æ˜¯è¶…è¶Š)

1. ç»†ç²’åº¦å®ä½“ç±»å‹é¢„æµ‹

ç»†ç²’åº¦å¯¹äºå­¦ä¹ åˆ°è¯çš„è¡¨å¾è¦æ±‚æé«˜äº†ä¸å°‘ï¼Œéœ€è¦æ¨¡å‹èƒ½åˆ†è¾¨å‡ºä¸Šä¸‹æ–‡ç»“æ„å¯¹è¯ä¹‰é€ æˆçš„å·®å¼‚.

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687927935-a0195a98-d678-4cd8-aaf8-9c5091bd2ead.png)

2. å¸¸è¯† QA å’Œå¼€æ”¾åŸŸ QA

å°è±¡é‡Œ,RoREATa åœ¨å¸¸è¯†é—®ç­”ä¸­æ¯” BERT Large èƒ½é«˜ 10 å¤šä¸ªç‚¹ï¼Œå¯¹æ¯” Multi-task çš„ç»“æœè™½ç„¶æå‡ä¸æ˜¯å¾ˆå¤§,ä½†è¿˜æ˜¯æœ‰æ˜æ˜¾çš„æå‡.

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687930982-e33e5549-331e-4d70-bb60-0785d37bae7a.png)

1. å…³ç³»åˆ†ç±»

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687932523-e0575150-672e-4633-b1c9-1096d8585135.png)

4. åˆºæ¢å®éªŒ: LAMA(å¸¸è¯†æ€§å¡«ç©ºé—®ç­”)

![image](https://cdn.nlark.com/yuque/0/2020/png/104214/1583687934134-84bcc45d-ac20-47a2-a67f-90d0135f18e1.png)

è™½ç„¶æ¯” RoBERTa æå‡æ˜æ˜¾ï¼Œä½†ç»“æœä½äº BERT Large.
æ–‡ä¸­è§£é‡Šåˆ°, ä¸»è¦æ˜¯å› ä¸º RoBERTa ä½¿ç”¨ byte-level çš„ BPE, è€Œ BERT ä½¿ç”¨ char-level çš„ BPE.(ä½†ä¸ºä»€ä¹ˆä¹‹å‰éƒ½éƒ½å¥½ï¼Œå°±åªæœ‰è¿™ä¸ªä»»åŠ¡ä¸Šä¼š)

## References

1. [K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters.][1]
2. [Parameter-Efï¬cient Transfer Learning for NLP.][2] ICML 2019.
3. [BERT and PALs: Projected Attention Layers for Efï¬cient Adaptation in Multi-Task Learning.][3] ICML 2019.
4. [BERT-A: Fine-tuning BERT with Adapters and Data Augmentation. CS224n 2019 FP.][4]
5. [ERNIE: Enhanced Language Representation with Informative Entities.][5] ACL 2019.
6. [Informing Unsupervised Pretraining with External Linguistic Knowledge.][6]
7. [Sensebert: Driving some sense into bert.][7]
8. [Knowledge Enhanced Contextual Word Representations.][8] EMNLP 2019.
9. [Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model.][9] ICLR 2020.
10. [Integrating Graph Contextualized Knowledge into Pre-trained Language Models.][10]

[1]: https://arxiv.org/abs/2002.01808
[2]: https://arxiv.org/abs/1902.00751
[3]: https://arxiv.org/abs/1902.02671
[4]: http://web.stanford.edu/class/cs224n/reports/default/15848417.pdf
[5]: https://arxiv.org/abs/1905.07129
[6]: https://arxiv.org/abs/1909.02339
[7]: https://arxiv.org/abs/1908.05646
[8]: https://arxiv.org/abs/1909.04164
[9]: https://arxiv.org/abs/1912.09637
[10]: https://arxiv.org/abs/1912.00147
